{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7aca2b6-f1dd-430b-8a93-64d2d6dfef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyvista\n",
    "#!pip install pyvista[jupyter]\n",
    "#!pip install pyvistaqt\n",
    "import imageio, pickle, uuid, os, time, shutil, tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "from pyvistaqt import BackgroundPlotter\n",
    "from skimage.measure import marching_cubes\n",
    "import nibabel as nib\n",
    "from nibabel.processing import resample_from_to\n",
    "from scipy.optimize import brentq\n",
    "from scipy.spatial import cKDTree, transform, ConvexHull, SphericalVoronoi\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from scipy.optimize import differential_evolution\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from tqdm.notebook import tqdm\n",
    "pv.set_jupyter_backend('none')    # if you want the trame-based UI\n",
    "import openxlab.dataset as oxlds\n",
    "import torch\n",
    "\n",
    "\n",
    "with open(\"C:/Users/colli/Downloads/filelist.txt\", \"r\", encoding='utf-8') as f:\n",
    "    filelist_str = f.read()\n",
    "filelist = filelist_str.split('|')\n",
    "filelist = filelist[7::3]\n",
    "filelist = [i.strip() for i in filelist]\n",
    "\n",
    "filedict = {}\n",
    "for path in filelist:  # e.g. ['/raw/a1/mesh/data1.obj', ...]\n",
    "    parts = path.strip(\"/\").split(\"/\")  # ['raw', 'a1', 'mesh', 'data1.obj']\n",
    "    current = filedict\n",
    "\n",
    "    for part in parts[:-1]:  # Traverse all but the last part\n",
    "        current = current.setdefault(part, {})  # Create if missing\n",
    "\n",
    "    # Add the final file name to a list at the deepest level\n",
    "    current[parts[-1].split('.')[0]] = path\n",
    "\n",
    "def download_openxlab(path):\n",
    "    oxlds.download(\"omniobject3d/OmniObject3D-New\", path, './data')\n",
    "    pathlist = path.strip('/').split('/')\n",
    "    # Path where it got saved\n",
    "    nested_path = \"./data/omniobject3d___OmniObject3D-New\" + path\n",
    "    \n",
    "    # Path where you want it\n",
    "    target_path = \"./data/\" + pathlist[-1].split('.')[0]\n",
    "\n",
    "    with tarfile.open(nested_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=target_path)\n",
    "        \n",
    "    # Optional: remove empty folders\n",
    "    shutil.rmtree(\"./data/omniobject3d___OmniObject3D-New/\")\n",
    "    print(f'File Downloaded and Extracted to {target_path}')\n",
    "    \n",
    "def print_data(mesh):\n",
    "    print(mesh.point_data)\n",
    "    print(mesh.cell_data)\n",
    "    print(mesh.field_data)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    path = Path(path)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    os.replace(tmp, path)  # atomic replace\n",
    "\n",
    "overall_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b559dc-7384-487c-b506-b23e05315427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edge_dict(mesh):\n",
    "    \"\"\"\n",
    "    Extract edge data from a mesh from vertex and face lists.\n",
    "    Returns a dictionary with each edge as a key in the form\n",
    "    of a sorted tuple, [(v0, v1)] if v0 < v1, [(v1, v0)] if v1 < v0.\n",
    "    Each key/edge will contain another dictionary with keys:\n",
    "    [opposite_vertex_0 : int, opposite_vertex_1 : int, \n",
    "     weight (w = cot(opposite_angle_0) + cot(opposite_angle_1)) : float]\n",
    "\n",
    "     Parameters\n",
    "     ----------\n",
    "     mesh : pv.PolyData\n",
    "     \n",
    "     Returns\n",
    "     -------\n",
    "     edge_dict : dictionary\n",
    "         Contains a dict for each edge [(v0,v1)] with keys opposite_vertex_0,\n",
    "         opposite_vertex_1, and weight. opposite_vertex_i contains\n",
    "         float of the angle in radians made by the three vertices\n",
    "         v0, opposite_vertex_i, v1\n",
    "    \"\"\"\n",
    "    verts = mesh.points\n",
    "    faces = mesh.faces\n",
    "    edge_dict = {}\n",
    "    if faces.ndim == 1:\n",
    "        faces = faces.reshape(-1, 4)\n",
    "    \n",
    "    for tri in faces[:,1:]:\n",
    "        i, j, k = map(int, tri)\n",
    "        for (a,b,c) in [(i,j,k),(j,k,i),(k,i,j)]:\n",
    "            e = tuple(sorted((a,b)))   # e.g. (min(a,b),max(a,b))\n",
    "            if e not in edge_dict:\n",
    "                edge_dict[e] = {}\n",
    "            v0 = verts[a]\n",
    "            v1 = verts[b]\n",
    "            v2 = verts[c]\n",
    "            e0 = v0 - v2\n",
    "            e1 = v1 - v2\n",
    "            costheta = np.dot(e0, e1) / (np.linalg.norm(e0) * np.linalg.norm(e1) + 1e-15)\n",
    "            costheta = np.clip(costheta, -1, 1)\n",
    "            angle = np.arccos(costheta)\n",
    "            edge_dict[e][c] = float(angle)     # store the vertex “across” this edge\n",
    "            if len(edge_dict[e]) == 2:\n",
    "                angle0 = list(edge_dict[e].items())[0][1]\n",
    "                w = (np.cos(angle0)/(np.sin(angle0) + 1e-15)) + (np.cos(angle)/(np.sin(angle) + 1e-15))\n",
    "                edge_dict[e]['weight'] = float(w)\n",
    "    edges_idx = np.array([(i[0], i[1]) for i in edge_dict], dtype=int)\n",
    "    weights = np.array([edge_dict[i].get('weight', 0.0) for i in edge_dict], dtype=float)\n",
    "\n",
    "    return edge_dict, edges_idx, weights\n",
    "\n",
    "def string_energy_vec(edges_idx, weights, phi):\n",
    "    # # phi: (N,3) array of vertex coords on the sphere\n",
    "    # phi = mesh['spherical_param']\n",
    "    # edges_idx = mesh['edges_idx']\n",
    "    # weights = mesh['edges_weights']\n",
    "    i0 = edges_idx[:,0]\n",
    "    i1 = edges_idx[:,1]\n",
    "    # shape (E,3) differences\n",
    "    diffs = phi[i0] - phi[i1]\n",
    "    # squared‐norm per edge, shape (E,)\n",
    "    sqnorm = np.einsum('ij,ij->i', diffs, diffs)\n",
    "    # sum w * sqnorm\n",
    "    return float(np.dot(weights, sqnorm))\n",
    "\n",
    "def extract_vertex_face_energies(\n",
    "    verts_sph: np.ndarray,\n",
    "    faces_pv: np.ndarray,\n",
    "    edges_idx: np.ndarray,\n",
    "    weights: np.ndarray,\n",
    "    *,\n",
    "    return_means: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute discrete Dirichlet energy contributions of a spherical parameterization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eE : (E,) per-edge energy w_ij * ||phi_i - phi_j||^2\n",
    "    fE : (F,) per-face sum of incident edge energies (split equally among incident faces)\n",
    "    vE : (V,) per-vertex sum of incident edge energies (split 0.5 to each endpoint)\n",
    "    (optionally) fE_mean, vE_mean : means normalized by incidence counts\n",
    "    \"\"\"\n",
    "    # faces as (F,3)\n",
    "    faces = faces_pv.reshape(-1, 4)[:, 1:] if faces_pv.ndim == 1 else faces_pv\n",
    "    V = verts_sph.shape[0]\n",
    "    F = faces.shape[0]\n",
    "    E = edges_idx.shape[0]\n",
    "\n",
    "    # 1) per-edge energy\n",
    "    i0, i1 = edges_idx[:, 0], edges_idx[:, 1]\n",
    "    diffs = verts_sph[i0] - verts_sph[i1]               # (E,3)\n",
    "    len2  = np.einsum('ij,ij->i', diffs, diffs)         # (E,)\n",
    "    eE    = weights * len2                               # (E,)\n",
    "\n",
    "    # 2) per-vertex accumulation (split evenly to endpoints)\n",
    "    vE = np.zeros(V, dtype=float)\n",
    "    np.add.at(vE, i0, 0.5 * eE)\n",
    "    np.add.at(vE, i1, 0.5 * eE)\n",
    "\n",
    "    # 3) per-face accumulation\n",
    "    # Build a light vertex->faces adjacency to find faces adjacent to each edge\n",
    "    v2f = [[] for _ in range(V)]\n",
    "    for fi, (a, b, c) in enumerate(faces):\n",
    "        v2f[a].append(fi); v2f[b].append(fi); v2f[c].append(fi)\n",
    "\n",
    "    fE = np.zeros(F, dtype=float)\n",
    "    face_inc_counts = np.zeros(F, dtype=int)\n",
    "\n",
    "    for k, (a, b) in enumerate(edges_idx):\n",
    "        adj = list(set(v2f[a]) & set(v2f[b]))  # 1 (boundary) or 2 (interior)\n",
    "        if len(adj) == 0:\n",
    "            # Non-manifold or disconnected edge; skip or assign nowhere\n",
    "            continue\n",
    "        share = eE[k] / len(adj)\n",
    "        for fi in adj:\n",
    "            fE[fi] += share\n",
    "            face_inc_counts[fi] += 1\n",
    "\n",
    "    if not return_means:\n",
    "        return eE, fE, vE\n",
    "\n",
    "    # 4) Optional: means normalized by incidence (avoid division by zero)\n",
    "    # For vertices, count how many incident edges per vertex\n",
    "    vertex_inc_counts = np.zeros(V, dtype=int)\n",
    "    np.add.at(vertex_inc_counts, i0, 1)\n",
    "    np.add.at(vertex_inc_counts, i1, 1)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        vE_mean = np.where(vertex_inc_counts > 0, vE / vertex_inc_counts, 0.0)\n",
    "        fE_mean = np.where(face_inc_counts   > 0, fE / face_inc_counts,     0.0)\n",
    "\n",
    "    return eE, fE_mean, vE_mean\n",
    "\n",
    "\n",
    "def d_energy_vec(edges_idx, weights, phi):\n",
    "    \"\"\"\n",
    "    Vectorized ∂E/∂Phi for all vertices at once.\n",
    "    \n",
    "    edges_idx : (E,2) int array of [i,j] pairs\n",
    "    weights   : (E,)  float array of w_ij\n",
    "    phi       : (N,3) float array of Phi positions\n",
    "\n",
    "    returns grad : (N,3) float array of 2*sum_j w_ij*(Phi_i - Phi_j)\n",
    "    \"\"\"\n",
    "    i0 = edges_idx[:, 0]      # shape = (E,)\n",
    "    i1 = edges_idx[:, 1]      # shape = (E,)\n",
    "\n",
    "    # 1) compute the per-edge vector differences\n",
    "    diffs    = phi[i0] - phi[i1]         # (E,3)\n",
    "\n",
    "    # 2) weight them\n",
    "    w_diffs  = diffs * weights[:, None]  # (E,3)\n",
    "\n",
    "    # 3) scatter‐add into a per-vertex accumulator\n",
    "    grad     = np.zeros_like(phi)        # (N,3)\n",
    "    np.add.at(grad, i0,  w_diffs)        # grad[i0] +=  w_diffs\n",
    "    np.add.at(grad, i1, -w_diffs)        # grad[i1] -=  w_diffs\n",
    "\n",
    "    # 4) if you want the true energy‐gradient, multiply by 2\n",
    "    # grad *= 2\n",
    "\n",
    "    return grad\n",
    "\n",
    "def C2_adaptive(phi0, edges_idx, weights, *,\n",
    "                tol=1e-10, max_iter=10_000, initial_dt=0.1, verbose=True):\n",
    "    \"\"\"\n",
    "    Projected gradient descent on S^2 to minimize sum w_ij ||phi_i - phi_j||^2.\n",
    "    Inputs:\n",
    "      phi0: (N,3) initial directions (will be normalized)\n",
    "      edges_idx: (E,2) int\n",
    "      weights: (E,) float\n",
    "    Returns:\n",
    "      phi: (N,3) final S^2 embedding\n",
    "      energies: list of energy values over iterations\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=max_iter, desc='Spherical Param Gradient Descent', unit='iter', dynamic_ncols=True)\n",
    "    # normalize just in case\n",
    "    phi = phi0 / (np.linalg.norm(phi0, axis=1, keepdims=True) + 1e-15)\n",
    "\n",
    "    E = string_energy_vec(edges_idx, weights, phi)\n",
    "    energies = [E]\n",
    "    dt = float(initial_dt)\n",
    "\n",
    "    for it in range(1, max_iter+1):\n",
    "        g = d_energy_vec(edges_idx, weights, phi)\n",
    "        # project gradient onto tangent space\n",
    "        g_proj = g - (np.einsum('ij,ij->i', g, phi))[:, None] * phi\n",
    "\n",
    "        ok = False\n",
    "        dt_try = dt\n",
    "        for _ in range(6):\n",
    "            phi_try = phi - dt_try * g_proj\n",
    "            phi_try /= (np.linalg.norm(phi_try, axis=1, keepdims=True) + 1e-15)\n",
    "            E_try = string_energy_vec(edges_idx, weights, phi_try)\n",
    "            if E_try < E:\n",
    "                phi, E = phi_try, E_try\n",
    "                energies.append(E)\n",
    "                dt = min(dt_try * 1.25, 1.0)\n",
    "                ok = True\n",
    "                break\n",
    "            dt_try *= 0.25\n",
    "\n",
    "        if not ok or (energies[-2] - energies[-1]) < tol:\n",
    "            break\n",
    "        if verbose:\n",
    "            pbar.update(1)\n",
    "    return phi, energies\n",
    "\n",
    "def normalize_mesh(mesh):\n",
    "    mesh_copy = mesh.copy()\n",
    "    mesh_copy.points -= mesh_copy.center_of_mass()\n",
    "    area = mesh_copy.compute_cell_sizes(area=True)['Area'].sum()\n",
    "    mesh_copy.points /= np.sqrt(area)\n",
    "    return mesh_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d47bcc4-e84f-485e-9853-6691ccbb3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def srnf_from_mesh(mesh):\n",
    "    mesh.compute_normals(point_normals=False, cell_normals=True, inplace=True)\n",
    "    mesh['Area'] = mesh.compute_cell_sizes(length=False, area=True, volume=False)['Area']\n",
    "    u = mesh.cell_data[\"Normals\"]                # (F, 3)\n",
    "    A = mesh.cell_data[\"Area\"]                  # (F,)\n",
    "    q = u * np.sqrt(2.0 * A)[:, None]           # (F, 3)\n",
    "    return q\n",
    "\n",
    "def spherical_triangle_area(a, b, c):\n",
    "    # a,b,c: (M,3) unit vectors; returns (M,) steradians\n",
    "    num = np.abs(np.einsum('ij,ij->i', a, np.cross(b, c)))\n",
    "    den = 1.0 + np.einsum('ij,ij->i', a, b) \\\n",
    "              + np.einsum('ij,ij->i', b, c) \\\n",
    "              + np.einsum('ij,ij->i', c, a)\n",
    "    return 2.0 * np.arctan2(num, den)\n",
    "\n",
    "def icosphere_quadrature(nsub=3, radius=1.0):\n",
    "    ico = pv.Icosphere(nsub=nsub, radius=radius)\n",
    "    faces = ico.faces\n",
    "    F = faces.reshape(-1, 4)[:, 1:]         # (M,3)\n",
    "    P = ico.points                               # (Nv,3) ~ unit vectors\n",
    "    vertices = P\n",
    "    tri = P[F]                                   # (M,3,3)\n",
    "    centers = tri.mean(axis=1)\n",
    "    centers /= np.linalg.norm(centers, axis=1, keepdims=True)\n",
    "    areas = spherical_triangle_area(tri[:,0], tri[:,1], tri[:,2]) * (radius**0)  # already on sphere\n",
    "    # sanity: areas.sum() ≈ 4π * (radius**2)\n",
    "    return centers, areas, vertices, faces\n",
    "\n",
    "def general_quadrature(P, F):\n",
    "    if F.ndim == 1:\n",
    "        F = F.reshape(-1,4)[:,1:]\n",
    "    tri = P[F]\n",
    "    centers = tri.mean(axis=1)\n",
    "    centers /= np.linalg.norm(centers, axis=1, keepdims=True)\n",
    "    areas = spherical_triangle_area(tri[:,0], tri[:,1], tri[:,2])\n",
    "    return centers, areas\n",
    "    \n",
    "def get_icosphere_level(A, B, nsub):\n",
    "    if nsub in A.icosphere:\n",
    "        return A.icosphere[nsub]['points'], A.icosphere[nsub]['weights'], A.icosphere[nsub]['vertices'], A.icosphere[nsub]['faces']\n",
    "    if nsub in B.icosphere:\n",
    "        return B.icosphere[nsub]['points'], B.icosphere[nsub]['weights'], B.icosphere[nsub]['vertices'], B.icosphere[nsub]['faces']\n",
    "    U, w, v, f = icosphere_quadrature(nsub)  # (M,3), (M,)\n",
    "    A.icosphere[nsub] = {'points': U, 'weights': w, 'vertices': v, 'faces': f}\n",
    "    B.icosphere[nsub] = {'points': U, 'weights': w, 'vertices': v, 'faces': f}\n",
    "    return U, w, v, f\n",
    "\n",
    "def get_sample_points(A, B):\n",
    "    # ensure alignments dicts exist\n",
    "    if getattr(A, \"alignments\", None) is None:\n",
    "        A.alignments = {}\n",
    "    if getattr(B, \"alignments\", None) is None:\n",
    "        B.alignments = {}\n",
    "\n",
    "    # if already cached on B for A, return it\n",
    "    if A.uid in B.alignments and 'sample_points' in B.alignments[A.uid]:\n",
    "        return B.alignments[A.uid]['sample_points']\n",
    "\n",
    "    # otherwise build once\n",
    "    sample_dict = build_sample_points(A, B)\n",
    "\n",
    "    # store symmetrically, creating nested dicts if needed\n",
    "    A.alignments.setdefault(B.uid, {})['sample_points'] = sample_dict\n",
    "    B.alignments.setdefault(A.uid, {})['sample_points'] = sample_dict\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "    \n",
    "def l2_q(qA, qB, w):\n",
    "    d = qA - qB\n",
    "    return float(np.dot(w, np.einsum('ij,ij->i', d, d)))\n",
    "\n",
    "# ... keep your helpers as provided above ...\n",
    "def l2(A: \"SquareRootNormalMesh\", B: \"SquareRootNormalMesh\", nsub = 5):\n",
    "    #sample_points, sample_weights, _, _ = get_icosphere_level(A, B, nsub)\n",
    "    sample_dict = get_sample_points(A,B)\n",
    "    sample_points = sample_dict['points']\n",
    "    sample_weights = sample_dict['weights']\n",
    "    if B.uid not in A.alignments.keys():\n",
    "        A.get_alignment(B)\n",
    "    mobius_parameters = A.alignments[B.uid]['mobius_params']\n",
    "    R = A.alignments[B.uid]['space_rotation']\n",
    "    sample_reparam, sqrtJ = mobius_apply_on_sphere(sample_points, mobius_parameters)\n",
    "\n",
    "    q1 = A.sample(sample_points)\n",
    "    gamma_q2 = B.sample(sample_reparam)\n",
    "    R_gamma_q2 = (gamma_q2 @ R.T) * sqrtJ[:, None]\n",
    "    \n",
    "    l2_sq = l2_q(q1, R_gamma_q2, sample_weights)\n",
    "\n",
    "    return l2_sq\n",
    "\n",
    "def stereographic_project(P):  # P: (N,3) unit vectors\n",
    "    X, Y, Z = P[:,0], P[:,1], P[:,2]\n",
    "    denom = 1.0 - Z\n",
    "    # guard near north pole: denom -> 0 => z -> ∞\n",
    "    denom = np.where(np.abs(denom) < 1e-15, 1e-15, denom)\n",
    "    return (X + 1j*Y) / denom  # complex array (N,)\n",
    "\n",
    "def stereographic_unproject(w):  # w: (N,) complex -> (N,3) unit vectors\n",
    "    x = np.real(w); y = np.imag(w)\n",
    "    r2 = x*x + y*y\n",
    "    denom = 1.0 + r2\n",
    "    P = np.stack([2*x/denom, 2*y/denom, (r2 - 1.0)/denom], axis=1)\n",
    "    # numeric safety\n",
    "    P /= np.linalg.norm(P, axis=1, keepdims=True)\n",
    "    return P\n",
    "\n",
    "def mobius_apply_on_sphere(P, mobius_params):\n",
    "    \"\"\"\n",
    "    P: (M,3) unit sphere points\n",
    "    returns: P_gamma (M,3), sqrtJ (M,)\n",
    "    \"\"\"\n",
    "    a, b, c, d = mobius_params\n",
    "    z = stereographic_project(P)            # (M,) complex\n",
    "    czd = c*z + d\n",
    "    w = (a*z + b) / (czd)               # (M,) complex\n",
    "    P_gamma = stereographic_unproject(w)    # (M,3)\n",
    "\n",
    "    # sqrt(J) on the sphere (see derivation in the message)\n",
    "    num = 1.0 + (z.real**2 + z.imag**2)\n",
    "    den = 1.0 + (w.real**2 + w.imag**2)\n",
    "    den2 = np.maximum(np.abs(czd)**2, 1e-15)\n",
    "    sqrtJ = (num/den) / den2      # (M,)\n",
    "    return P_gamma, sqrtJ\n",
    "\n",
    "# ---------------------------\n",
    "# Low-discrepancy sampling on SO(3)\n",
    "# (Halton-based; named \"sobol\" for API compatibility)\n",
    "# ---------------------------\n",
    "def _van_der_corput(n, base, skip=0, shift=0.0):\n",
    "    i = np.arange(skip+1, skip+n+1, dtype=float)\n",
    "    seq = np.zeros_like(i)\n",
    "    denom = 1.0\n",
    "    while np.any(i > 0):\n",
    "        i, rem = divmod(i, base)\n",
    "        denom *= base\n",
    "        seq += rem / denom\n",
    "    return (seq + shift) % 1.0\n",
    "\n",
    "def sample_quaternions_sobol(N, skip=0, shifts=(0.0,0.0,0.0)):\n",
    "    u1 = _van_der_corput(N, 2, skip=skip, shift=shifts[0])\n",
    "    u2 = _van_der_corput(N, 3, skip=skip, shift=shifts[1])\n",
    "    u3 = _van_der_corput(N, 5, skip=skip, shift=shifts[2])\n",
    "    # ... Shoemake as before ...\n",
    "\n",
    "    theta1 = 2*np.pi*u2\n",
    "    theta2 = 2*np.pi*u3\n",
    "    r1 = np.sqrt(1.0 - u1)\n",
    "    r2 = np.sqrt(u1)\n",
    "    w = r2*np.cos(theta2)\n",
    "    x = r1*np.sin(theta1)\n",
    "    y = r1*np.cos(theta1)\n",
    "    z = r2*np.sin(theta2)\n",
    "    Q = np.stack([w,x,y,z], axis=1)\n",
    "    # numeric guard\n",
    "    Q /= np.linalg.norm(Q, axis=1, keepdims=True) + 1e-15\n",
    "    return Q\n",
    "\n",
    "def su2_from_quat(w, x, y, z):\n",
    "    \"\"\"\n",
    "    Map unit quaternion to SU(2) matrix [[α, β],[-β*, α*]].\n",
    "    \"\"\"\n",
    "    alpha = w + 1j*z\n",
    "    beta  = y + 1j*x\n",
    "    s = np.sqrt((alpha*alpha.conjugate()).real + (beta*beta.conjugate()).real)\n",
    "    alpha /= (s + 1e-15)\n",
    "    beta  /= (s + 1e-15)\n",
    "    return np.array([[alpha, beta],\n",
    "                     [-beta.conjugate(), alpha.conjugate()]], dtype=np.complex128)\n",
    "\n",
    "# ---------------------------\n",
    "# Möbius plumbing\n",
    "# ---------------------------\n",
    "def compose_similarity_then_rotation(lam, t, Mrot):\n",
    "    \"\"\"\n",
    "    Msim = [[lam, lam*t], [0, 1]], then M = Msim @ Mrot.\n",
    "    Returns a,b,c,d (complex scalars).\n",
    "    \"\"\"\n",
    "    Msim = np.array([[lam, lam*t],\n",
    "                     [0+0j, 1+0j]], dtype=np.complex128)\n",
    "    M = Msim @ Mrot\n",
    "    a, b = M[0,0], M[0,1]\n",
    "    c, d = M[1,0], M[1,1]\n",
    "    # normalize (det ~ 1), optional but helps stability\n",
    "    det = a*d - b*c\n",
    "    if np.abs(det) > 1e-15:\n",
    "        s = np.sqrt(det)\n",
    "        a /= s; b /= s; c /= s; d /= s\n",
    "    return a, b, c, d\n",
    "\n",
    "# ---------------------------\n",
    "# SRNF alignment helpers\n",
    "# ---------------------------\n",
    "def kabsch_srnf(Q1, Q2, weights=None, proper_rotation=True):\n",
    "    \"\"\"\n",
    "    Weighted Kabsch on 3D vectors (rows correspond).\n",
    "    Returns R (3x3).\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        w = np.ones(len(Q1), float)\n",
    "    else:\n",
    "        w = np.asarray(weights, float)\n",
    "    w = w / (w.sum() + 1e-15)\n",
    "    C = Q1.T @ (Q2 * w[:, None])\n",
    "    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n",
    "    R = U @ Vt\n",
    "    if proper_rotation and np.linalg.det(R) < 0:\n",
    "        U[:, -1] *= -1\n",
    "        R = U @ Vt\n",
    "    return R\n",
    "\n",
    "def face_centroids_on_sphere(mesh_sph: pv.PolyData) -> np.ndarray:\n",
    "    F = mesh_sph.faces.reshape(-1, 4)[:, 1:]     # (nF, 3) vertex indices\n",
    "    P = mesh_sph.points                           # (nV, 3)\n",
    "    C = P[F].mean(axis=1)                         # (nF, 3) Euclidean centroids\n",
    "    # ensure they lie on S^2 (good for meshes slightly off the unit sphere)\n",
    "    C /= (np.linalg.norm(C, axis=1, keepdims=True) + 1e-15)\n",
    "    return C\n",
    "\n",
    "def build_face_kdtree(mesh_sph: pv.PolyData):\n",
    "    C = face_centroids_on_sphere(mesh_sph)       # (nF, 3)\n",
    "    tree = cKDTree(C)\n",
    "    return tree, C\n",
    "\n",
    "def wrap_angle(a):\n",
    "    a = np.mod(a, 2*np.pi)\n",
    "    return a\n",
    "\n",
    "def zoom_range(center, half_width, n, is_angle=False):\n",
    "    if is_angle:\n",
    "        grid = np.linspace(-half_width, +half_width, n)\n",
    "        return wrap_angle(center + grid)\n",
    "    else:\n",
    "        return np.linspace(center - half_width, center + half_width, n)\n",
    "\n",
    "def locate_on_mesh_from_sphere(A: \"SquareRootNormalMesh\", U: np.ndarray):\n",
    "    \"\"\"\n",
    "    U: (M,3) unit directions on S^2.\n",
    "    Returns:\n",
    "      P_world: (M,3) interpolated points on A.mesh (original geometry)\n",
    "      face_ids: (M,) triangle indices on A.mesh_sph\n",
    "      bary: (M,3) barycentric weights (for optional reuse)\n",
    "    \"\"\"\n",
    "    P_sph = A.mesh_sph.points\n",
    "    F = A.mesh_sph.faces.reshape(-1,4)[:,1:]\n",
    "    face_ids = A.preindex(U)                 # (M,)\n",
    "    tri_idx = F[face_ids]                    # (M,3) vertex ids\n",
    "    a_s, b_s, c_s = P_sph[tri_idx[:,0]], P_sph[tri_idx[:,1]], P_sph[tri_idx[:,2]]\n",
    "\n",
    "    # barycentrics in the chord triangle (good for small faces)\n",
    "    # vectorized computation\n",
    "    v0 = b_s - a_s\n",
    "    v1 = c_s - a_s\n",
    "    v2 = U   - a_s\n",
    "    d00 = np.einsum('ij,ij->i', v0, v0)\n",
    "    d01 = np.einsum('ij,ij->i', v0, v1)\n",
    "    d11 = np.einsum('ij,ij->i', v1, v1)\n",
    "    d20 = np.einsum('ij,ij->i', v2, v0)\n",
    "    d21 = np.einsum('ij,ij->i', v2, v1)\n",
    "    denom = d00 * d11 - d01 * d01 + 1e-15\n",
    "    w2 = (d00 * d21 - d01 * d20) / denom\n",
    "    w1 = (d11 * d20 - d01 * d21) / denom\n",
    "    w0 = 1.0 - w1 - w2\n",
    "    bary = np.stack([w0, w1, w2], axis=1)    # (M,3)\n",
    "\n",
    "    # (optional) clamp tiny negatives due to numeric error\n",
    "    # bary = np.clip(bary, 0.0, 1.0); bary /= bary.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # interpolate original-geometry point positions\n",
    "    V_world = A.mesh.points                  # (N,3) original mesh vertices\n",
    "    pa, pb, pc = V_world[tri_idx[:,0]], V_world[tri_idx[:,1]], V_world[tri_idx[:,2]]\n",
    "    P_world = (bary[:,[0]] * pa +\n",
    "               bary[:,[1]] * pb +\n",
    "               bary[:,[2]] * pc)\n",
    "\n",
    "    return P_world\n",
    "\n",
    "#def get_equal_area_sample_points(A: \"SquareRootNormalMesh\", B: \"SquareRootNormalMesh\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbec8ed9-990e-4646-b93e-aa6e2770a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "def unit(X): return X / (np.linalg.norm(X, axis=1, keepdims=True)+EPS)\n",
    "def build_sample_points(A, B, nsub=5):\n",
    "    type_error = TypeError(\"A and B must both be type ndarray or SquareRootNormalMesh!\")\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A_samp = A\n",
    "        if isinstance(B, np.ndarray):\n",
    "            B_samp = B\n",
    "        elif B is None:\n",
    "            B_samp = np.empty((0,3), float)\n",
    "        else:\n",
    "            raise type_error\n",
    "    elif hasattr(A, 'mesh_sph'):\n",
    "        A_samp = general_quadrature(A.mesh_sph.points, A.mesh_sph.faces)[0]\n",
    "        if hasattr(B, 'mesh_sph'):\n",
    "            B_samp = general_quadrature(B.mesh_sph.points, B.mesh_sph.faces)[0]\n",
    "        elif B is None:\n",
    "            B_samp = np.empty((0,3), float)\n",
    "        else:\n",
    "            raise type_error\n",
    "    else:\n",
    "        raise type_error\n",
    "    ico_samp = pv.Icosphere(nsub=nsub).points if nsub>=0 else np.empty((0,3), float)\n",
    "    samples = np.vstack([A_samp, B_samp, ico_samp])\n",
    "    def dedupe_on_sphere(samples, eps=1e-10):\n",
    "        S = samples / np.linalg.norm(samples, axis=1)[:,None]\n",
    "        tree = cKDTree(S)\n",
    "        parent = np.arange(len(S))\n",
    "        def find(a):\n",
    "            while parent[a] != a:\n",
    "                parent[a] = parent[parent[a]]\n",
    "                a = parent[a]\n",
    "            return a\n",
    "        def union(a,b):\n",
    "            ra, rb = find(a), find(b)\n",
    "            if ra != rb: parent[rb] = ra\n",
    "        for a,b in tree.query_pairs(eps):\n",
    "            union(a,b)\n",
    "        roots = np.array([find(i) for i in range(len(S))])\n",
    "        uniq_roots, labels = np.unique(roots, return_inverse=True)\n",
    "        uniq  = np.zeros((len(uniq_roots),3))\n",
    "        counts = np.zeros(len(uniq_roots), dtype=int)\n",
    "        for k,r in enumerate(uniq_roots):\n",
    "            idx = np.where(roots==r)[0]\n",
    "            uniq[k] = unit(S[idx].mean(axis=0, keepdims=True))[0]\n",
    "            #uniq = S[idx] / np.linalg.norm(S[idx])\n",
    "            counts[k] = len(idx)\n",
    "        return uniq, labels, counts\n",
    "    \n",
    "    def sort_region_vertices_ccw(region_idx, vertices, site):\n",
    "        \"\"\"Sort polygon vertices CCW around 'site' on S^2 in a tangent frame.\"\"\"\n",
    "        pts = vertices[np.asarray(region_idx, dtype=int)]\n",
    "        pts = unit(pts)\n",
    "        n = unit(site[None,:])[0]\n",
    "        # build orthonormal tangent basis (t1 along first edge, t2 = n x t1)\n",
    "        t1 = unit((pts[0] - n*np.dot(n, pts[0]))[None,:])[0]\n",
    "        t2 = np.cross(n, t1)\n",
    "        # angles\n",
    "        x = pts @ t1\n",
    "        y = pts @ t2\n",
    "        ang = np.arctan2(y, x)\n",
    "        order = np.argsort(ang)\n",
    "        return np.asarray(region_idx, dtype=int)[order]\n",
    "    \n",
    "    def spherical_polygon_area(sorted_poly):\n",
    "        \"\"\"Girard on already-ordered polygon.\"\"\"\n",
    "        poly = unit(sorted_poly)\n",
    "        m = len(poly)\n",
    "        A = 0.0\n",
    "        for i in range(m):\n",
    "            u = poly[(i-1)%m]; v = poly[i]; w = poly[(i+1)%m]\n",
    "            n = v\n",
    "            def proj(x):\n",
    "                x = x - n*np.dot(n, x)\n",
    "                return x / (np.linalg.norm(x)+EPS)\n",
    "            a = proj(u); b = proj(w)\n",
    "            num = np.linalg.norm(np.cross(a,b)); den = np.clip(np.dot(a,b), -1.0, 1.0)\n",
    "            A += np.arctan2(num, den)\n",
    "        return A - (m-2)*np.pi\n",
    "    \n",
    "    def voronoi_weights(samples, eps=1e-10):\n",
    "        uniq, labels, counts = dedupe_on_sphere(samples, eps=eps)\n",
    "        print(\"Sample points dropped:\", samples.shape[0] - uniq.shape[0])\n",
    "        sv = SphericalVoronoi(uniq.astype(np.float64),\n",
    "                              radius=1.0,\n",
    "                              center=np.array([0.0,0.0,0.0], dtype=np.float64),\n",
    "                              threshold=eps*10)\n",
    "    \n",
    "        # Manually sort each region CCW in a tangent frame at the generator\n",
    "        areas_uniq = np.empty(len(uniq))\n",
    "        for i, reg in enumerate(sv.regions):        \n",
    "            if len(reg) < 3:\n",
    "                areas_uniq[i] = 0\n",
    "                continue\n",
    "            reg_sorted = sort_region_vertices_ccw(reg, sv.vertices, uniq[i])\n",
    "            poly = sv.vertices[reg_sorted]\n",
    "            areas_uniq[i] = spherical_polygon_area(poly)\n",
    "    \n",
    "        weights_per_site = areas_uniq / (4*np.pi)\n",
    "        weights = weights_per_site[labels] / counts[labels]\n",
    "        weights = np.clip(weights, 0.0, None)\n",
    "        weights /= weights.sum()\n",
    "        return weights, areas_uniq, labels, counts, uniq\n",
    "    out = voronoi_weights(samples)\n",
    "    return {'points': out[4], 'weights': out[0]*4*np.pi}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GenLogger:\n",
    "    def __init__(self, base_func, pop_size):\n",
    "        self.base_func = base_func\n",
    "        self.pop_size = pop_size\n",
    "        self.eval_count = 0\n",
    "        self.failure_count = 0\n",
    "        self.cur_gen_best = float('inf')\n",
    "        self.history = []   # list of (gen_idx, best_energy)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        val = self.base_func(x)\n",
    "        self.eval_count += 1\n",
    "        if val > 999:\n",
    "            self.failure_count += 1\n",
    "        if val < self.cur_gen_best:\n",
    "            self.cur_gen_best = val\n",
    "        # end of generation?\n",
    "        if self.eval_count % self.pop_size == 0:\n",
    "            gen_idx = self.eval_count // self.pop_size\n",
    "            self.history.append((gen_idx, self.cur_gen_best))\n",
    "            # reset for next gen\n",
    "            self.cur_gen_best = float('inf')\n",
    "        return val\n",
    "\n",
    "def project_psl2c(a,b,c,d, eps=1e-12):\n",
    "    det = a*d - b*c\n",
    "    if not np.isfinite(det.real + det.imag) or abs(det) <= eps:\n",
    "        return None\n",
    "    # complex sqrt; +0j forces complex dtype\n",
    "    k = 1.0 / np.sqrt(det + 0j)\n",
    "    # optional: guard against insane scaling (rare but helpful)\n",
    "    if abs(k) > 1e6:   # tune as you like\n",
    "        return None\n",
    "    return a*k, b*k, c*k, d*k\n",
    "\n",
    "def coarse_candidates(A: \"SquareRootNormalMesh\", B: \"SquareRootNormalMesh\", nsub=5,\n",
    "                      param_range_array=None, max_iter=1000, popsize=15, mutation=(0.5,1),\n",
    "                      recombination=0.7, seed = None, REL_TOL=0.01, JMIN=1e-6, JMAX=1e6,\n",
    "                      MAXLOGVAR=4):\n",
    "    \"\"\"\n",
    "    Return topK candidates sorted by L2: each item is\n",
    "    (L2, (qw,qx,qy,qz), s, th, r, phi, a,b,c,d, Rspace)\n",
    "    \"\"\"\n",
    "\n",
    "    #U, w, verts, faces = get_icosphere_level(A, B, nsub)\n",
    "    sample_dict = build_sample_points(A, B)\n",
    "    U = sample_dict['points']\n",
    "    w = sample_dict['weights']\n",
    "    # initial_icosphere_points = verts.copy()\n",
    "    # initial_icosphere_faces = faces.reshape(-1,4)[:,1:]\n",
    "    if param_range_array is None:\n",
    "        c0 = np.repeat(-1, 8).reshape(-1,1)\n",
    "        c1 = np.repeat(1, 8).reshape(-1,1)\n",
    "        param_range_array = np.hstack([c0,c1])\n",
    "\n",
    "    def robust_calculation(mobius_params):\n",
    "        U2 = mobius_apply_on_sphere(initial_icosphere_points, mobius_params)[0]          # map VERTICES\n",
    "        U2 = U2 / (np.linalg.norm(U2, axis=1, keepdims=True) + 1e-15)  # <-- normalize vertices\n",
    "        C2, A1 = general_quadrature(U2, initial_icosphere_faces)                   # mapped centers + areas\n",
    "        sqrtJ  = np.sqrt(A1 / (w + 1e-15))                          # w == A0\n",
    "        return C2, sqrtJ\n",
    "\n",
    "\n",
    "    FOUR_PI = 4.0*np.pi\n",
    "    \n",
    "    def check_mobius(w, sqrtJ):\n",
    "        J = (sqrtJ**2)\n",
    "        S = float((w * J).sum())\n",
    "        J_logvar = np.var(np.log(J + 1e-10))\n",
    "        return (np.isfinite(S)\n",
    "                and abs(S - FOUR_PI) <= REL_TOL * FOUR_PI\n",
    "                and (J > JMIN).all() and (J < JMAX).all()\n",
    "                and J_logvar < MAXLOGVAR)\n",
    "        \n",
    "\n",
    "    if seed is None:\n",
    "        np.random.seed(np.random.randint(10000))\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "    q1 = A.sample(U)\n",
    "    B_tree, C = build_face_kdtree(B.mesh_sph)\n",
    "\n",
    "    # [a, ai, b, bi, c, ci, d, di]\n",
    "    def score(params):\n",
    "        p = np.array(params, copy=True)\n",
    "        a = complex(params[0], params[1])\n",
    "        b = complex(params[2], params[3])\n",
    "        c = complex(params[4], params[5])\n",
    "        d = complex(params[6], params[7])\n",
    "        proj = project_psl2c(a,b,c,d)\n",
    "        if proj is None:\n",
    "            return 1e3\n",
    "        a,b,c,d = proj\n",
    "\n",
    "        U2, sqrtJ = mobius_apply_on_sphere(U, (a,b,c,d))\n",
    "        if not check_mobius(w, sqrtJ):\n",
    "            return 1e3\n",
    "            # U2, sqrtJ = robust_calculation((a,b,c,d))\n",
    "            # if not check_mobius(w, sqrtJ):\n",
    "            #     return 1e3\n",
    "        _, idx = B_tree.query(U2)\n",
    "        gamma_q2 = B.q_face[idx] * sqrtJ[:,None]\n",
    "        R = kabsch_srnf(q1, gamma_q2, w, proper_rotation=True)\n",
    "        R_gamma_q2 = gamma_q2 @ R.T\n",
    "        L2 = l2_q(q1, R_gamma_q2, w)\n",
    "        return L2\n",
    "\n",
    "    pbar = tqdm(total=max_iter, desc='DE Iteration', unit='gen', dynamic_ncols=True)\n",
    "    gen = {\"i\": 0}\n",
    "    def cb(xk, convergence):\n",
    "        gen[\"i\"] += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    for i in range(15):\n",
    "        if ((popsize * 8) / 2**i) < 1:\n",
    "            sobol_size = 2**i\n",
    "            break\n",
    "    scorer = GenLogger(score, sobol_size)\n",
    "\n",
    "    pool = ThreadPool()\n",
    "    \n",
    "    result = differential_evolution(\n",
    "        scorer,\n",
    "        bounds=param_range_array,\n",
    "        strategy=\"best1bin\",\n",
    "        maxiter=max_iter,\n",
    "        popsize=popsize,\n",
    "        mutation=mutation,       # (min, max) → jitter range\n",
    "        recombination=recombination,         # aka crossover probability\n",
    "        tol=1e-6,\n",
    "        seed=seed,\n",
    "        workers=pool.map,                # parallel evaluation\n",
    "        updating=\"deferred\",       # better with workers>1\n",
    "        polish=True,               # local L-BFGS-B polish at the end\n",
    "        init=\"sobol\",              # Sobol’ or \"latinhypercube\" are good\n",
    "        callback=cb\n",
    "    )\n",
    "    pool.close();pool.join()\n",
    "    result.population_energies = scorer.history\n",
    "    print(f\"Total: {scorer.eval_count} Failed: {scorer.failure_count}\")\n",
    "    return result\n",
    "\n",
    "def compute_alignment(A: \"SquareRootNormalMesh\", B: \"SquareRootNormalMesh\", nsub: int=5):\n",
    "    result = coarse_candidates(A, B, nsub=nsub)\n",
    "\n",
    "    l2 = result.fun\n",
    "    params = result.x\n",
    "    log = result.population_energies\n",
    "    a = complex(params[0], params[1])\n",
    "    b = complex(params[2], params[3])\n",
    "    c = complex(params[4], params[5])\n",
    "    d = complex(params[6], params[7])\n",
    "\n",
    "    proj = project_psl2c(a,b,c,d)\n",
    "    a,b,c,d = proj\n",
    "    \n",
    "    #U, w, v, f = get_icosphere_level(A, B, nsub)\n",
    "    sample_dict = build_sample_points(A, B)\n",
    "    U = sample_dict['points']\n",
    "    w = sample_dict['weights']\n",
    "    U2, sqrtJ = mobius_apply_on_sphere(U, (a,b,c,d))\n",
    "    q1 = A.sample(U)\n",
    "    gamma_q2 = B.sample(U2) * sqrtJ[:, None]\n",
    "    space_rotation = kabsch_srnf(q1, gamma_q2, w, proper_rotation=True)\n",
    "    robust_l2 = l2_q(q1, (gamma_q2 @ space_rotation.T), w)\n",
    "    if np.abs(l2 - robust_l2) > 1e-5:\n",
    "        print(\"L2 mismatch greater than 1e-5\")\n",
    "    return (a,b,c,d), space_rotation, robust_l2, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74210a53-31e9-4050-a106-d5ac76bf2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalars_polar_hue(U, sat_max=0.9, gamma=0.65, p=2):\n",
    "    \"\"\"\n",
    "    North pole -> white, south pole -> black.\n",
    "    Brightness V from z (with gamma for contrast).\n",
    "    Saturation increases toward equator: S = sat_max*(1-|z|^p).\n",
    "    Hue from longitude for orientation cues.\n",
    "    \"\"\"\n",
    "    x, y, z = U.T\n",
    "    # HSV components\n",
    "    H = (np.arctan2(y, x) + np.pi) / (2*np.pi)          # [0,1) longitude\n",
    "    V = np.clip(0.5*(z + 1.0), 0, 1) ** gamma            # brightness: south→0, north→1\n",
    "    S = sat_max * (1.0 - np.abs(z)**p)                   # low at poles, high near equator\n",
    "\n",
    "    # HSV -> RGB (vectorized, standard)\n",
    "    C = V * S\n",
    "    h6 = H * 6.0\n",
    "    i = np.floor(h6).astype(int)\n",
    "    f = h6 - i\n",
    "    X = C * (1.0 - np.abs((h6 % 2.0) - 1.0))\n",
    "    Z = np.zeros_like(C)\n",
    "\n",
    "    rgbp = np.zeros((len(U), 3), float)\n",
    "    i_mod = i % 6\n",
    "    rgbp[i_mod==0] = np.stack([C, X, Z], axis=1)[i_mod==0]\n",
    "    rgbp[i_mod==1] = np.stack([X, C, Z], axis=1)[i_mod==1]\n",
    "    rgbp[i_mod==2] = np.stack([Z, C, X], axis=1)[i_mod==2]\n",
    "    rgbp[i_mod==3] = np.stack([Z, X, C], axis=1)[i_mod==3]\n",
    "    rgbp[i_mod==4] = np.stack([X, Z, C], axis=1)[i_mod==4]\n",
    "    rgbp[i_mod==5] = np.stack([C, Z, X], axis=1)[i_mod==5]\n",
    "\n",
    "    m = (V - C)[:, None]\n",
    "    rgb = np.clip(rgbp + m, 0, 1)\n",
    "    return (255*rgb).astype(np.uint8)\n",
    "\n",
    "def scalars_oct(P):\n",
    "    octant = (P[:,0] > 0).astype(int)*4 + (P[:,1] > 0).astype(int)*2 + (P[:,2] > 0).astype(int)\n",
    "    return octant\n",
    "    # colormap: 'tab10' or 'tab20'\n",
    "\n",
    "def scalars_fifty(P, seed = None):\n",
    "    x = (P[:,0] > np.sqrt(3)/3).astype(int)*4 + (P[:,0] > 0).astype(int)*2 + (P[:,0] > -np.sqrt(3)/3).astype(int)\n",
    "    y = (P[:,1] > np.sqrt(3)/3).astype(int)*32 + (P[:,1] > 0).astype(int)*16 + (P[:,1] > -np.sqrt(3)/3).astype(int)*8\n",
    "    z = (P[:,2] > np.sqrt(3)/3).astype(int)*256 + (P[:,2] > 0).astype(int)*128 + (P[:,2] > -np.sqrt(3)/3).astype(int)*64\n",
    "    uni, cats = np.unique(x+y+z, return_inverse=True)\n",
    "    K = len(uni)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(K)\n",
    "    return perm[cats]\n",
    "\n",
    "def plot_registration_one_panel(A: \"SquareRootNormalMesh\", B: \"SquareRootNormalMesh\", nsub=5,\n",
    "                                cmap='tab20', scalars_method='hue', hue=(1, 0.05, 15), mobius_params=None,\n",
    "                                space_rotation = None, gap=(2,0,0), point_size = 5, scalars_array=None):\n",
    "    if B.uid not in A.alignments:\n",
    "        raise BaseException('Mesh B not in Mesh A alignment dict')\n",
    "    mobius_params = A.alignments[B.uid]['mobius_params'] if mobius_params is None else mobius_params\n",
    "    space_rotation = A.alignments[B.uid]['space_rotation'] if space_rotation is None else space_rotation\n",
    "    def _barycentric_weights(p, a, b, c):\n",
    "        \"\"\"Planar barycentrics of p in triangle (a,b,c) (all 3D).\"\"\"\n",
    "        v0, v1, v2 = b - a, c - a, p - a\n",
    "        d00 = np.dot(v0, v0); d01 = np.dot(v0, v1); d11 = np.dot(v1, v1)\n",
    "        d20 = np.dot(v2, v0); d21 = np.dot(v2, v1)\n",
    "        denom = d00 * d11 - d01 * d01 + 1e-15\n",
    "        v = (d11 * d20 - d01 * d21) / denom\n",
    "        w = (d00 * d21 - d01 * d20) / denom\n",
    "        u = 1.0 - v - w\n",
    "        return np.array([u, v, w])\n",
    "    \n",
    "    #icosphere_points = pv.Icosphere(nsub=nsub).points\n",
    "    sample_dict = build_sample_points(A,B)\n",
    "    icosphere_points = sample_dict['points']\n",
    "    a_ico_mapping = A.locate(icosphere_points)\n",
    "    gamma_icosphere_points = mobius_apply_on_sphere(icosphere_points, mobius_params)[0]\n",
    "    b_ico_mapping = B.locate(gamma_icosphere_points)\n",
    "\n",
    "    # --- A ---\n",
    "    a_mesh = A.mesh.copy()\n",
    "    a_points = a_mesh.points\n",
    "\n",
    "    a_R = kabsch_srnf(icosphere_points, a_ico_mapping, proper_rotation=True)\n",
    "    a_points = a_points @ a_R.T\n",
    "    a_ico_mapping = a_ico_mapping @ a_R.T\n",
    "    \n",
    "    scale = np.max(np.linalg.norm(a_points, axis=1))\n",
    "    a_points *= 2 / scale\n",
    "    a_ico_mapping *= 2 / scale\n",
    "    \n",
    "    a_points -= np.array(gap)\n",
    "    a_ico_mapping  -= np.array(gap)\n",
    "    a_mesh.points = a_points\n",
    "\n",
    "    # --- B ---\n",
    "    b_mesh = B.mesh.copy()\n",
    "    b_points = b_mesh.points\n",
    "    b_points = b_points @ space_rotation.T\n",
    "    b_ico_mapping = b_ico_mapping @ space_rotation.T\n",
    "    b_R = kabsch_srnf(icosphere_points, b_ico_mapping, proper_rotation=True)\n",
    "    b_points = b_points @ b_R\n",
    "    b_ico_mapping = b_ico_mapping @ b_R\n",
    "\n",
    "    scale = np.max(np.linalg.norm(b_points, axis=1))\n",
    "    b_points *= 2 / scale\n",
    "    b_ico_mapping *= 2 / scale\n",
    "    \n",
    "    b_ico_mapping += np.array(gap)\n",
    "    b_points += np.array(gap)\n",
    "    b_mesh.points = b_points\n",
    "\n",
    "    jo = 0\n",
    "    while True:\n",
    "        if (np.min(np.linalg.norm(a_points, axis = 1)) < 0.9**(jo // 5)) or (np.min(np.linalg.norm(b_points, axis = 1)) < 0.9**(jo // 5)):\n",
    "            rotation = transform.Rotation.random().as_matrix()\n",
    "            a_points += np.array(gap)\n",
    "            a_points @= rotation.T\n",
    "            a_points -= np.array(gap)\n",
    "            b_points -= np.array(gap)\n",
    "            b_points @= rotation.T\n",
    "            b_points += np.array(gap)\n",
    "            a_ico_mapping = (a_ico_mapping + np.array(gap)) @ rotation.T - np.array(gap)\n",
    "            b_ico_mapping = (b_ico_mapping - np.array(gap)) @ rotation.T + np.array(gap)\n",
    "            icosphere_points = icosphere_points @ rotation.T\n",
    "            jo += 1\n",
    "            if jo % 5 == 0:\n",
    "                icosphere_points *= 0.9\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    if scalars_method == 'hue':\n",
    "        scalars = scalars_polar_hue(icosphere_points, sat_max=hue[0], gamma=hue[1], p=hue[2])\n",
    "\n",
    "        p = BackgroundPlotter()\n",
    "        p.add_mesh(a_mesh, color = 'black')\n",
    "        p.add_mesh(a_ico_mapping, scalars=scalars, rgb=True, point_size=point_size)\n",
    "\n",
    "        p.add_mesh(b_mesh, color = 'black')\n",
    "        p.add_mesh(b_ico_mapping, scalars=scalars, rgb=True, point_size=point_size)\n",
    "\n",
    "        p.add_mesh(icosphere_points, scalars=scalars, rgb=True, point_size=point_size)\n",
    "        plot_registration_one_panel.plotter = p\n",
    "        return 1\n",
    "    elif scalars_method == 'fifty':\n",
    "        scalars = scalars_fifty(icosphere_points)\n",
    "    elif scalars_method == 'oct':\n",
    "        scalars = scalars_oct(icosphere_points)\n",
    "    elif scalars_array is None:\n",
    "        scalars = np.arange(a_mesh.n_cells)\n",
    "    else:\n",
    "        scalars = scalars_array\n",
    "        \n",
    "    p = BackgroundPlotter()\n",
    "    p.add_mesh(a_mesh, color = 'black')\n",
    "    p.add_mesh(a_ico_mapping, cmap=cmap, scalars=scalars, point_size=point_size)\n",
    "    \n",
    "    p.add_mesh(b_mesh, color = 'black')\n",
    "    p.add_mesh(b_ico_mapping, cmap=cmap, scalars=scalars, point_size=point_size)\n",
    "    \n",
    "    p.add_mesh(icosphere_points, cmap=cmap, scalars=scalars, point_size=point_size)\n",
    "    p.show()\n",
    "    plot_registration_one_panel.plotter = p\n",
    "    return 1\n",
    "\n",
    "def gif(A: \"SquareRootNormalMesh\", B=None, camera_position=None, n_frames=300,\n",
    "        fps=30, cmap='tab20c', scalars_array=None, scalars_method='hue', point=None, hue=(1, 0.05, 15),\n",
    "        out_path=\"overwritten_file.gif\", wire=False, spin=False):\n",
    "    if out_path[:11] != 'animations/':\n",
    "        out_path = 'animations/' + out_path\n",
    "    camera_position = [(3.862980654462277, 3.8625836577257506, 3.86289431713432),\n",
    "                       (0.00011098384857177734, -0.0002860128879547119, 2.4646520614624023e-05),\n",
    "                       (0.0, 0.0, 1.0)] if camera_position is None else camera_position\n",
    "    mesh1 = A.mesh\n",
    "    mesh2 = A.mesh_sph if B is None else B\n",
    "\n",
    "    def f_exp(t, a):\n",
    "        return (np.exp(a*t) - 1) / (np.exp(a) - 1)\n",
    "\n",
    "    def find_a(x0, y0, K=50.0):\n",
    "        if not (0 < y0 < 1):\n",
    "            raise ValueError(\"y0 must lie strictly between 0 and 1\")\n",
    "        if abs(y0 - x0) < 1e-8:\n",
    "            return 0.0\n",
    "        def fn(a):\n",
    "            return f_exp(x0, a) - y0\n",
    "\n",
    "        a_star = brentq(fn, -K, K)\n",
    "        return a_star\n",
    "\n",
    "    a = 1e-8 if point is None else find_a(*point)\n",
    "    diffeo = lambda t: f_exp(t, a)\n",
    "\n",
    "    if scalars_method == 'hue':\n",
    "        scalar_points = general_quadrature(A.mesh_sph.points, A.mesh_sph.faces.reshape(-1,4)[:,1:])[0]\n",
    "        scalars = scalars_polar_hue(scalar_points, sat_max=hue[0], gamma=hue[1], p=hue[2])\n",
    "    elif scalars_method == 'fifty':\n",
    "        scalars = scalars_fifty(mesh1.points)\n",
    "    elif scalars_method == 'oct':\n",
    "        scalars = scalars_oct(mesh1.points)\n",
    "    elif scalars_array is None:\n",
    "        cmap = \"PiYG\" if cmap == 'tab20c' else cmap\n",
    "        scalars = np.arange(mesh1.n_cells)\n",
    "    else:\n",
    "        cmap = \"PiYG\" if cmap == 'tab20c' else cmap\n",
    "        scalars = scalars_array\n",
    "    style = 'wireframe' if wire else 'surface'\n",
    "        \n",
    "    working = mesh1.copy()\n",
    "    working['cell_ids'] = scalars\n",
    "\n",
    "    plotter = pv.Plotter(off_screen=True)\n",
    "    if scalars_method != 'hue':\n",
    "        plotter.add_mesh(working,\n",
    "                         scalars='cell_ids',\n",
    "                         cmap=cmap,\n",
    "                         show_scalar_bar=False,\n",
    "                         smooth_shading=False,\n",
    "                         style=style)\n",
    "    else:\n",
    "        plotter.add_mesh(working,\n",
    "                         scalars='cell_ids',\n",
    "                         rgb=True,\n",
    "                         show_scalar_bar=False,\n",
    "                         smooth_shading=False,\n",
    "                         style=style)\n",
    "    plotter.camera_position = camera_position\n",
    "\n",
    "    frames = []\n",
    "    if spin:\n",
    "        for t in np.linspace(0, 1, n_frames):\n",
    "            plotter.camera.Azimuth(360/n_frames)\n",
    "            plotter.render()\n",
    "            img = plotter.screenshot(return_img=True)\n",
    "            frames.append(img)\n",
    "\n",
    "    for t in np.linspace(0, 1, n_frames):\n",
    "        ft = diffeo(t)\n",
    "        working.points = (1 - ft) * mesh1.points + ft * mesh2.points\n",
    "        if spin:\n",
    "            plotter.camera.Azimuth(360/n_frames)\n",
    "        plotter.render()\n",
    "        img = plotter.screenshot(return_img=True)\n",
    "        frames.append(img)\n",
    "\n",
    "    file_saved_message = f\"Animation Save at {out_path}\"\n",
    "    if out_path[-3:] == 'mp4':\n",
    "        imageio.mimsave(out_path, frames, format = \"FFMPEG\", fps = fps, codec = \"libx264\", quality = 8)\n",
    "        print(file_saved_message)\n",
    "    elif out_path[-3:] == 'gif':\n",
    "        imageio.mimsave(out_path, frames, fps = fps)\n",
    "        print(file_saved_message)\n",
    "    else:\n",
    "        print('Frames returned. Please use valid file type.')\n",
    "        return frames            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822cfc1b-aaf9-4273-a27e-b3659afbfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SquareRootNormalMesh:\n",
    "    verts: np.ndarray\n",
    "    verts_sph: np.ndarray\n",
    "    faces_pv:  np.ndarray\n",
    "    q_face:    np.ndarray\n",
    "    name: str = \"\"\n",
    "    uid:  str = field(default_factory=lambda: uuid.uuid4().hex)\n",
    "\n",
    "    mesh: pv.PolyData = field(init=False, repr=False)\n",
    "    mesh_sph: pv.PolyData = field(init=False, repr=False)\n",
    "    mesh_initial: pv.PolyData = field(init=False, repr=False)\n",
    "\n",
    "    edge_dict: dict = field(init=False, repr=False)\n",
    "    edge_array: np.ndarray = field(init=False, repr=False)\n",
    "    edge_weights: np.ndarray = field(init=False, repr=False)\n",
    "\n",
    "    sphere_param_edge_energy: np.ndarray = field(init=False, repr=False)\n",
    "    sphere_param_face_energy_mean: np.ndarray = field(init=False, repr=False)\n",
    "    sphere_param_vertex_energy_mean: np.ndarray = field(init=False, repr=False)\n",
    "    sphere_param_log: list = field(default_factory=list)\n",
    "    icosphere: dict = field(default_factory=dict)\n",
    "    alignments: dict = field(default_factory=dict)\n",
    "    pairwise_l2: dict = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.mesh_sph = pv.PolyData(self.verts_sph, self.faces_pv)\n",
    "\n",
    "    @classmethod\n",
    "    def from_polydata(cls, initial_mesh: pv.PolyData, *, max_iter=5000, name=\"\"):\n",
    "        # translate and scale so mesh centroid is (0,0,0) and mesh surface area is 1\n",
    "        pv_mesh = normalize_mesh(initial_mesh)\n",
    "        \n",
    "        # build edges/weights\n",
    "        edge_dict, edge_array, edge_weight = make_edge_dict(pv_mesh)\n",
    "\n",
    "        # ensure point normals exist and normalize as φ0 ∈ S²\n",
    "        m = pv_mesh.copy()\n",
    "        m.compute_normals(point_normals=True, cell_normals=False, inplace=True)\n",
    "        phi0 = m.point_normals\n",
    "        phi0 = phi0 / (np.linalg.norm(phi0, axis=1, keepdims=True) + 1e-15)\n",
    "\n",
    "        # optimize on S²\n",
    "        verts_sph, logE = C2_adaptive(phi0, edge_array, edge_weight, max_iter=max_iter)\n",
    "        R = kabsch_srnf(m.points, verts_sph, proper_rotation=False)\n",
    "        if np.linalg.det(R) < 0:\n",
    "            print('Inverse Rotation')\n",
    "        verts_sph = verts_sph @ R.T\n",
    "\n",
    "        faces_pv = pv_mesh.faces\n",
    "        q_face = srnf_from_mesh(pv_mesh)\n",
    "\n",
    "        # construct\n",
    "        obj = cls(\n",
    "            verts=pv_mesh.points.copy(),\n",
    "            verts_sph=verts_sph,\n",
    "            faces_pv=faces_pv.copy(),\n",
    "            q_face=q_face,\n",
    "            name=name\n",
    "        )\n",
    "        obj.edge_dict = edge_dict\n",
    "        obj.edge_array = edge_array\n",
    "        obj.edge_weights = edge_weight\n",
    "        obj.mesh = pv_mesh\n",
    "        obj.mesh_sph = pv.PolyData(verts_sph, faces_pv)\n",
    "        obj.mesh_initial = initial_mesh\n",
    "\n",
    "        # energies (NOTE: your function aggregates weights, not energy)\n",
    "        eE, fE, vE = extract_vertex_face_energies(verts_sph, faces_pv, edge_array, edge_weight)\n",
    "        obj.sphere_param_edge_energy = eE\n",
    "        obj.sphere_param_face_energy_mean = fE\n",
    "        obj.sphere_param_vertex_energy_mean = vE\n",
    "        obj.sphere_param_log = logE\n",
    "        return obj\n",
    "\n",
    "    @classmethod\n",
    "    def from_voxels(cls, vox: np.ndarray, *, iso=0.5, spacing=(1,1,1), smoothing=None, **kw):\n",
    "        from skimage.measure import marching_cubes\n",
    "        v,f,_,_ = marching_cubes(vox, level=iso, spacing=spacing)\n",
    "        faces_pv = np.c_[np.full((len(f),1),3,np.int32), f.astype(np.int32)].ravel()\n",
    "        pv_mesh = pv.PolyData(v, faces_pv)\n",
    "        if smoothing: pv_mesh = pv_mesh.smooth(n_iter=smoothing)\n",
    "        return cls.from_polydata(pv_mesh, **kw)\n",
    "\n",
    "    def sample(self, U: np.ndarray) -> np.ndarray:\n",
    "        idx = self.mesh_sph.find_closest_cell(U)\n",
    "        return self.q_face[idx]\n",
    "\n",
    "    def preindex(self, U: np.ndarray) -> np.ndarray:\n",
    "        return self.mesh_sph.find_closest_cell(U)\n",
    "\n",
    "    def gather(self, idx: np.ndarray) -> np.ndarray:\n",
    "        return self.q_face[idx]\n",
    "\n",
    "    def locate(self, U: np.ndarray) -> np.ndarray:\n",
    "        return locate_on_mesh_from_sphere(self, U)\n",
    "\n",
    "    def get_alignment(self, B: \"SquareRootNormalMesh\", nsub: int = 5, inplace: bool = True) -> None:\n",
    "        output = compute_alignment(self, B, nsub)\n",
    "        if not inplace:\n",
    "            if B.uid in self.alignments:\n",
    "                return output\n",
    "            else:\n",
    "                self.alignments[B.uid] = {'mobius_params': output[0], 'space_rotation': output[1], \"logs\": output[3]}\n",
    "                self.pairwise_l2[B.uid] = output[2]\n",
    "                B.pairwise_l2[B.uid] = output[2]\n",
    "        else:\n",
    "            self.alignments[B.uid] = {'mobius_params': output[0], 'space_rotation': output[1], \"logs\": output[3]}\n",
    "            self.pairwise_l2[B.uid] = output[2]\n",
    "            B.pairwise_l2[self.uid] = output[2]\n",
    "\n",
    "    def plot(self, scalars: np.ndarray = None, cmap: str = \"coolwarm\", sphere: bool = False) -> None:\n",
    "        if scalars is None:\n",
    "            scalars = np.arange(self.verts.shape[0])\n",
    "        plot_mesh = self.mesh_sph if sphere else self.mesh\n",
    "        p = BackgroundPlotter()\n",
    "        p.add_mesh(plot_mesh, cmap = cmap, scalars = scalars)\n",
    "        p.show()\n",
    "\n",
    "    def plot_registration(self, B: \"SquareRootNormalMesh\", nsub: int=7, scalars_method: str='hue',\n",
    "                            cmap: str='viridis', gap: tuple=(2,0,0), point_size: int=5) -> None:\n",
    "        if B.uid not in self.alignments:\n",
    "            raise ValueError(\"Mesh B is not aligned with mesh A!\")\n",
    "\n",
    "        mobius_params = self.alignments[B.uid]['mobius_params']\n",
    "        space_rotation = self.alignments[B.uid]['space_rotation'].T\n",
    "\n",
    "        plot_registration_one_panel(self, B, nsub, cmap, scalars_method, mobius_params=mobius_params,\n",
    "                                    space_rotation=space_rotation, gap=gap, point_size=point_size)\n",
    "\n",
    "    def plot_sphere_gif(self, camera_position=None, n_frames=300, fps=30, cmap='tab20c', scalars_array=None,\n",
    "                       scalars_method='hue', point=None, hue=(1,0.05,15), out_path='overwritten_file.gif',\n",
    "                       wire=False, spin=False):\n",
    "        gif(self, camera_position=camera_position, n_frames=n_frames, fps=fps, cmap=cmap, scalars_array=scalars_array,\n",
    "           scalars_method=scalars_method, point=point, hue=hue, out_path=out_path, wire=wire, spin=spin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e635fdd-cceb-43cc-8ef0-ceeca522f7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f90d72e8a0245ffa29a86c9c1e30dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Spherical Param Gradient Descent:   0%|                                                     | 0/8000 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Rotation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f0c25185d5417eb9881ced01901c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Spherical Param Gradient Descent:   0%|                                                    | 0/14000 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make meshes: 281.84\n"
     ]
    }
   ],
   "source": [
    "d1_file = \"C:/Users/colli/Documents/Thesis/data/d1/d1.nii\"\n",
    "d1_image = nib.load(d1_file)\n",
    "d1_mat = d1_image.get_fdata()\n",
    "d1_seg_file = \"C:/Users/colli/Documents/Thesis/data/d1/seg/segmented_seg.nii\"\n",
    "d1_seg_image = nib.load(d1_seg_file)\n",
    "d1_seg_mat = d1_seg_image.get_fdata()\n",
    "\n",
    "mask_file = \"//wsl.localhost/Ubuntu-22.04/home/colli/jo/corpus_callosum_mask.nii.gz\"\n",
    "mask_image = nib.load(mask_file)\n",
    "mask_resample = nib.processing.resample_from_to(mask_image, d1_seg_image)\n",
    "mask_mat = mask_resample.get_fdata()\n",
    "mask_mat[np.where(mask_mat < 0.5)] = 0\n",
    "mask_mat[np.where(mask_mat >= 0.5)] = 1\n",
    "\n",
    "verts, faces, normals, values = marching_cubes(mask_mat)\n",
    "col = np.full((faces.shape[0], 1), 3)\n",
    "raw_cc_mesh = pv.PolyData(verts, np.hstack((col, faces)).flatten())\n",
    "cc_mesh = raw_cc_mesh.smooth(n_iter = 500, relaxation_factor = 0.005)\n",
    "\n",
    "cc_srnm = SquareRootNormalMesh.from_polydata(cc_mesh, max_iter=8000)\n",
    "\n",
    "def prep_for_param(mesh: pv.PolyData, target_tris=None, seal=True) -> pv.PolyData:\n",
    "    m = mesh.triangulate().clean().compute_normals(inplace=False)\n",
    "    if seal:\n",
    "        m = m.fill_holes(1e6).extract_largest().clean()\n",
    "    # optional: decimate or subdivide to a comfortable triangle count\n",
    "    if target_tris is not None:\n",
    "        if m.n_cells > target_tris:\n",
    "            frac = 1 - target_tris / m.n_cells\n",
    "            m = m.decimate_pro(fraction=frac, preserve_topology=True)\n",
    "        else:\n",
    "            # Loop subdivision for smoother sampling\n",
    "            iters = max(1, int(np.ceil(np.log2(target_tris / m.n_cells))))\n",
    "            m = m.subdivide(nsub=iters, subfilter='loop')\n",
    "    return m.compute_normals(inplace=False)\n",
    "\n",
    "shark_mesh = pv.examples.download_great_white_shark()\n",
    "shark_mesh = prep_for_param(shark_mesh)\n",
    "\n",
    "shark_srnm = SquareRootNormalMesh.from_polydata(shark_mesh, max_iter=14000)\n",
    "\n",
    "create_mesh_end = time.time()\n",
    "goober = create_mesh_end - overall_start\n",
    "print(f\"To make meshes: {goober:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2294fc-df5c-4ca0-bd1e-3ff52dfdb4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cow_srnm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m cow_mesh \u001b[38;5;241m=\u001b[39m prep_for_param(cow_mesh)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#cow_srnm = SquareRootNormalMesh.from_polydata(cow_mesh, max_iter=20000)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mcow_srnm\u001b[49m\u001b[38;5;241m.\u001b[39mget_alignment(shark_srnm)\n\u001b[0;32m     28\u001b[0m cow_srnm\u001b[38;5;241m.\u001b[39mplot_registration(shark_srnm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cow_srnm' is not defined"
     ]
    }
   ],
   "source": [
    "def prep_for_param(mesh: pv.PolyData, target_tris=None, seal=True) -> pv.PolyData:\n",
    "    m = mesh.triangulate().clean().compute_normals(inplace=False)\n",
    "    if seal:\n",
    "        m = m.fill_holes(1e6).extract_largest().clean()\n",
    "    # optional: decimate or subdivide to a comfortable triangle count\n",
    "    if target_tris is not None:\n",
    "        if m.n_cells > target_tris:\n",
    "            frac = 1 - target_tris / m.n_cells\n",
    "            m = m.decimate_pro(fraction=frac, preserve_topology=True)\n",
    "        else:\n",
    "            # Loop subdivision for smoother sampling\n",
    "            iters = max(1, int(np.ceil(np.log2(target_tris / m.n_cells))))\n",
    "            m = m.subdivide(nsub=iters, subfilter='loop')\n",
    "    return m.compute_normals(inplace=False)\n",
    "\n",
    "shark_mesh = pv.examples.download_great_white_shark()\n",
    "shark_mesh = prep_for_param(shark_mesh)\n",
    "\n",
    "#shark_srnm = SquareRootNormalMesh.from_polydata(shark_mesh, max_iter=20000)\n",
    "\n",
    "cow_mesh = pv.examples.download_cow()\n",
    "cow_mesh = prep_for_param(cow_mesh)\n",
    "\n",
    "#cow_srnm = SquareRootNormalMesh.from_polydata(cow_mesh, max_iter=20000)\n",
    "\n",
    "cow_srnm.get_alignment(shark_srnm)\n",
    "\n",
    "cow_srnm.plot_registration(shark_srnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35f7a1ce-5e92-4fdf-a451-b74c863f0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_point(pts, faces, idx0, idx1, og_point, edges=None, n_iter=5000):\n",
    "    if faces.ndim == 1:\n",
    "        faces = faces.reshape(-1, 4)[:, 1:]\n",
    "    if edges is None:\n",
    "        edges = build_edges(faces)\n",
    "\n",
    "    # --- 1. Identify Neighbors (Same as your code) ---\n",
    "    idx_pair = np.array([idx0, idx1])\n",
    "    # Find faces connected to the pair\n",
    "    rel_face_mask = np.any(np.isin(faces, idx_pair), axis=1)\n",
    "    rel_faces = faces[rel_face_mask]\n",
    "    \n",
    "    # Calculate radius based on ring (Same as your code)\n",
    "    ring = edges[np.any(np.isin(edges, idx_pair), axis=1)].flatten()\n",
    "    ring = np.unique(ring[~np.isin(ring, idx_pair)])\n",
    "    maxy = np.max(np.linalg.norm(og_point - pts[ring], axis=1))\n",
    "\n",
    "    # --- 2. Orientation Logic (Kept your logic, but risky!) ---\n",
    "    other_faces = faces[~rel_face_mask]\n",
    "    centers = np.mean(pts[other_faces], axis=1)\n",
    "    cross_other = get_cross(pts, other_faces) \n",
    "    # Normalized dot product check\n",
    "    dots = np.einsum('ij,ij->i', centers, cross_other) # assuming get_cross is unnormalized\n",
    "    if np.mean(dots) < 0:\n",
    "        rel_faces = rel_faces[:, [0, 2, 1]] # Flip winding order\n",
    "\n",
    "    # --- 3. Vectorized Sampling ---\n",
    "    # Generate all random parameters at once\n",
    "    r0 = maxy * np.sqrt(np.random.uniform(0, 1, n_iter))\n",
    "    r1 = maxy * np.sqrt(np.random.uniform(0, 1, n_iter))\n",
    "    phi0 = np.random.uniform(0, 2 * np.pi, n_iter)\n",
    "    phi1 = np.random.uniform(0, 2 * np.pi, n_iter)\n",
    "\n",
    "    # Tangent Basis Construction\n",
    "    o = og_point / (np.linalg.norm(og_point) + 1e-10)\n",
    "    a = np.array([1.0, 0.0, 0.0]) if abs(o[0]) < 0.9 else np.array([0.0, 1.0, 0.0])\n",
    "    e0 = np.cross(o, a); e0 /= np.linalg.norm(e0)\n",
    "    e1 = np.cross(o, e0)\n",
    "\n",
    "    # Create Sample Offsets (n_iter, 3)\n",
    "    samp0 = r0[:, None] * (np.cos(phi0)[:, None] * e0 + np.sin(phi0)[:, None] * e1)\n",
    "    samp1 = r1[:, None] * (np.cos(phi1)[:, None] * e0 + np.sin(phi1)[:, None] * e1)\n",
    "\n",
    "    # Candidate positions on sphere\n",
    "    cands0 = og_point + samp0\n",
    "    cands0 /= np.linalg.norm(cands0, axis=1, keepdims=True)\n",
    "    cands1 = og_point + samp1\n",
    "    cands1 /= np.linalg.norm(cands1, axis=1, keepdims=True)\n",
    "\n",
    "    # --- 4. Vectorized Evaluation ---\n",
    "    # We need to compute metrics for 'rel_faces' for all 5000 candidates.\n",
    "    # We create a tensor of points: (n_iter, n_rel_faces, 3, 3) \n",
    "    # or just gather indices.\n",
    "    \n",
    "    # Map global indices to 0, 1 (for idx0, idx1) and 2..N (for static neighbors)\n",
    "    # This part is tricky to vectorize without fancy indexing, \n",
    "    # but here is the cleanest way:\n",
    "    \n",
    "    # Create a copy of pts for the static geometry\n",
    "    local_pts = pts.copy() \n",
    "    \n",
    "    # We need to construct the triangle vertices for all candidates.\n",
    "    # Shape: (n_iter, n_rel_faces, 3 coordinates, 3 xyz)\n",
    "    # This requires constructing a gathered array where we swap in cands0 and cands1\n",
    "    \n",
    "    # Fast approach: Look at rel_faces. \n",
    "    # Replace idx0 with a special flag -1, idx1 with -2\n",
    "    mapped_faces = rel_faces.copy()\n",
    "    mask0 = (mapped_faces == idx0)\n",
    "    mask1 = (mapped_faces == idx1)\n",
    "    \n",
    "    # Prepare a tensor of shape (n_iter, n_rel_faces, 3, 3)\n",
    "    # Fill with static points first\n",
    "    # (Note: This is memory intensive. If faces > 1000, do batches. \n",
    "    # But usually rel_faces is small ~10 faces).\n",
    "    num_rel = len(rel_faces)\n",
    "    \n",
    "    # Get static coords for all vertices in relative faces\n",
    "    base_coords = pts[rel_faces] # (n_rel, 3, 3)\n",
    "    batch_coords = np.tile(base_coords[None, ...], (n_iter, 1, 1, 1)) # (n_iter, n_rel, 3, 3)\n",
    "    \n",
    "    # Inject candidate 0\n",
    "    # mask0 is (n_rel, 3) -> expand to (n_iter, n_rel, 3)\n",
    "    batch_coords[:, mask0, :] = cands0.repeat(np.sum(mask0), axis=0).reshape(n_iter, -1, 3)\n",
    "    \n",
    "    # Inject candidate 1\n",
    "    batch_coords[:, mask1, :] = cands1.repeat(np.sum(mask1), axis=0).reshape(n_iter, -1, 3)\n",
    "\n",
    "    # --- 5. Compute Metrics on Tensor ---\n",
    "    # v0, v1, v2 for cross product\n",
    "    v0 = batch_coords[:, :, 0, :]\n",
    "    v1 = batch_coords[:, :, 1, :]\n",
    "    v2 = batch_coords[:, :, 2, :]\n",
    "    \n",
    "    # Cross product (normals * 2*area)\n",
    "    # Shape: (n_iter, n_rel, 3)\n",
    "    cross_vecs = np.cross(v1 - v0, v2 - v0)\n",
    "    cross_norms = np.linalg.norm(cross_vecs, axis=2) + 1e-10\n",
    "    \n",
    "    # Centers\n",
    "    tri_centers = np.mean(batch_coords, axis=2)\n",
    "    tri_centers_norm = np.linalg.norm(tri_centers, axis=2) + 1e-10\n",
    "    \n",
    "    # Dots (Outward check)\n",
    "    # (n_iter, n_rel)\n",
    "    dot_vals = np.einsum('ijk,ijk->ij', tri_centers/tri_centers_norm[...,None], cross_vecs/cross_norms[...,None])\n",
    "    \n",
    "    mean_dots = np.mean(dot_vals, axis=1) # (n_iter,)\n",
    "    \n",
    "    # CV Area\n",
    "    cv_areas = np.std(cross_norms, axis=1) / np.mean(cross_norms, axis=1) # (n_iter,)\n",
    "    \n",
    "    # Final Metric\n",
    "    final_metrics = mean_dots - (cv_areas / 3)\n",
    "    \n",
    "    best_idx = np.argmax(final_metrics)\n",
    "    \n",
    "    return {idx0: cands0[best_idx], idx1: cands1[best_idx]}\n",
    "\n",
    "def append_three_col(F):\n",
    "    return np.hstack([np.repeat(3, F.shape[0])[:,None], F])\n",
    "\n",
    "def remove_unused_points(P, F):\n",
    "    if F.ndim == 1:\n",
    "        F = F.reshape(-1,4)[:,1:]\n",
    "    unico = np.unique(F)\n",
    "    new_P = P[unico]\n",
    "    new_F = np.searchsorted(unico, F)\n",
    "    return new_P, new_F, unico\n",
    "    \n",
    "def ordered_one_ring(v, faces):\n",
    "    faces_v = faces[np.any(faces == v, axis=1)]\n",
    "    n, p = faces_v.shape\n",
    "    cols = np.arange(p)\n",
    "    rows = np.arange(n)[:, None]\n",
    "    \n",
    "    pos = np.where(faces_v == v)[1]      # original positions of 100\n",
    "    target = 2                        # column where you want 100 to end up\n",
    "    \n",
    "    shifts = (target - pos) % p       # per-row shift amounts\n",
    "    col_shift = (cols - shifts[:,None]) % p\n",
    "    \n",
    "    out = faces_v[rows, col_shift]\n",
    "\n",
    "    out2 = np.zeros((n,p), dtype=faces.dtype)\n",
    "    out2[0] = out[0]\n",
    "    col1 = out[:,0]\n",
    "    for i in range(1,n):\n",
    "        x = out2[i-1, 1]\n",
    "        idx = np.where(col1 == x)[0]\n",
    "        if len(idx) != 1:\n",
    "            print('Bad Connectivity')\n",
    "            return np.unique(out[:,:2])\n",
    "        out2[i] = out[idx[0]]\n",
    "    return out2[:,0]\n",
    "    \n",
    "def split_vertex_insert(current_faces, insert_tuple, replace_tuple, S=None, return_replaced_idx=False):\n",
    "    \"\"\"\n",
    "    Insert two deleted faces back (by row), rewire vertex ids using recorded (rows, cols),\n",
    "    then compute ordered rings for the two restored vertices.\n",
    "    Optionally place them on S² using a normalized centroid.\n",
    "    \"\"\"\n",
    "    # --- 1) Insert the two faces back by ROW (axis=0), descending index first\n",
    "    i0, f0 = insert_tuple[0]    # row indices to insert at\n",
    "    i1, f1 = insert_tuple[1]    # the two 3-idx faces to insert\n",
    "    if i0 > i1:\n",
    "        i0, i1, f0, f1 = i1, i0, f1, f0  # insert high row first\n",
    "    current_faces = np.insert(current_faces, i0, f0, axis=0)\n",
    "    current_faces = np.insert(current_faces, i1, f1, axis=0)\n",
    "\n",
    "    # --- 2) Rewire recorded (row, col) positions back to original vertex ids\n",
    "    # replace_tuple = [ ( (rows0, cols0), vid0 ), ( (rows1, cols1), vid1 ) ]\n",
    "    (rows0, cols0), vid0 = replace_tuple[0]\n",
    "    (rows1, cols1), vid1 = replace_tuple[1]\n",
    "    if return_replaced_idx:\n",
    "        uniq = np.unique(np.concat([current_faces[rows0, cols0], current_faces[rows1, cols1]]))\n",
    "        replaced_idx = [i for i in uniq if (i != vid0) and (i != vid1)][0]\n",
    "    current_faces[rows0, cols0] = vid0\n",
    "    current_faces[rows1, cols1] = vid1\n",
    "\n",
    "    # --- 3) Build ordered 1-rings for the two restored vertices\n",
    "    ring0 = ordered_one_ring(vid0, current_faces)\n",
    "    ring1 = ordered_one_ring(vid1, current_faces)\n",
    "\n",
    "    # --- 4) Optional: initialize spherical positions for vid0/vid1\n",
    "    if S is not None:\n",
    "        S[vid0] = spherical_kernel_centroid(ring0, S)\n",
    "        S[vid1] = spherical_kernel_centroid(ring1, S)\n",
    "\n",
    "    if vid1 not in ring0:\n",
    "        ring0 = np.append(ring0, vid1)\n",
    "    if vid0 not in ring1:\n",
    "        ring1 = np.append(ring1, vid0)\n",
    "\n",
    "    if return_replaced_idx:\n",
    "        return current_faces, ring0, ring1, replaced_idx\n",
    "    else:\n",
    "        return current_faces, ring0, ring1\n",
    "        \n",
    "def get_cross(points, faces):\n",
    "    if faces.ndim == 1:\n",
    "        faces = faces.reshape(-1,4)[:,1:]\n",
    "    e0 = points[faces[:,1]] - points[faces[:,0]]\n",
    "    e1 = points[faces[:,2]] - points[faces[:,0]]\n",
    "    cross = np.cross(e0, e1)\n",
    "    return cross\n",
    "    \n",
    "def build_edges(faces):\n",
    "    if faces.ndim == 1:\n",
    "        faces = faces.reshape(-1, 4)[:, 1:]\n",
    "    edges = np.vstack([\n",
    "        faces[:, [0, 1]],\n",
    "        faces[:, [1, 2]],\n",
    "        faces[:, [2, 0]],\n",
    "    ])\n",
    "    edges = np.unique(np.sort(edges, axis=1), axis=0)\n",
    "    return edges\n",
    "\n",
    "def cotan_weights_for_edge_list(\n",
    "    edges_idx: np.ndarray,\n",
    "    points: np.ndarray,\n",
    "    faces: np.ndarray,\n",
    "    *,\n",
    "    boundary_weight: float = 0.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cotangent weights for a provided edge list (E,2), returning weights\n",
    "    in the SAME order as edges_idx.\n",
    "\n",
    "    Weight for an interior edge (u,v):\n",
    "        w_uv = cot(angle at opp0) + cot(angle at opp1)\n",
    "    where opp0/opp1 are the two vertices opposite (u,v) in the two incident triangles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    edges_idx : (E,2) int array\n",
    "        Edge endpoints. Can be directed or undirected; treated as undirected.\n",
    "    points : (V,3) float array\n",
    "        Vertex positions in R^3 (the geometry you measure angles on).\n",
    "    faces : (F,3) or (F,4) or (4F,) array\n",
    "        Triangles. If PyVista style, may be (F,4) with leading 3 or flat (4F,) array.\n",
    "    boundary_weight : float\n",
    "        Weight assigned to boundary/non-manifold edges that do not have exactly 2 incident triangles.\n",
    "        Common choices: 0.0 (default), or cot(single angle) if you want 1-sided weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : (E,) float array\n",
    "        Cotan weights aligned with edges_idx row order.\n",
    "    \"\"\"\n",
    "    edges_idx = np.asarray(edges_idx)\n",
    "    points = np.asarray(points)\n",
    "    faces = np.asarray(faces)\n",
    "\n",
    "    # ---- normalize faces to (F,3) int array ----\n",
    "    if faces.ndim == 1:\n",
    "        # PyVista flat format: [3,i,j,k, 3,i,j,k, ...]\n",
    "        faces = faces.reshape(-1, 4)\n",
    "    if faces.shape[1] == 4:\n",
    "        # PyVista (F,4): [3,i,j,k]\n",
    "        faces = faces[:, 1:]\n",
    "    faces = faces.astype(np.int64, copy=False)\n",
    "\n",
    "    # ---- canonicalize edges and build lookup to provided order ----\n",
    "    edges_can = np.sort(edges_idx.astype(np.int64, copy=False), axis=1)\n",
    "    edge_to_pos = {tuple(e): i for i, e in enumerate(map(tuple, edges_can))}\n",
    "\n",
    "    E = edges_idx.shape[0]\n",
    "    cot_sum = np.zeros(E, dtype=np.float64)\n",
    "    cot_cnt = np.zeros(E, dtype=np.int8)\n",
    "\n",
    "    # ---- helper: cot(angle at vertex c) in triangle (a,b,c), opposite edge (a,b) ----\n",
    "    def cot_angle_at_vertex(c: int, a: int, b: int) -> float:\n",
    "        v0 = points[a] - points[c]\n",
    "        v1 = points[b] - points[c]\n",
    "        # cot(theta) = dot(v0,v1) / ||cross(v0,v1)||\n",
    "        denom = np.linalg.norm(np.cross(v0, v1)) + 1e-15\n",
    "        return float(np.dot(v0, v1) / denom)\n",
    "\n",
    "    # ---- single pass over faces: accumulate cotangents into edges present in edges_idx ----\n",
    "    for (i, j, k) in faces:\n",
    "        # edge (i,j) opposite k\n",
    "        e = (i, j) if i < j else (j, i)\n",
    "        pos = edge_to_pos.get(e)\n",
    "        if pos is not None:\n",
    "            cot_sum[pos] += cot_angle_at_vertex(k, i, j)\n",
    "            cot_cnt[pos] += 1\n",
    "\n",
    "        # edge (j,k) opposite i\n",
    "        e = (j, k) if j < k else (k, j)\n",
    "        pos = edge_to_pos.get(e)\n",
    "        if pos is not None:\n",
    "            cot_sum[pos] += cot_angle_at_vertex(i, j, k)\n",
    "            cot_cnt[pos] += 1\n",
    "\n",
    "        # edge (k,i) opposite j\n",
    "        e = (k, i) if k < i else (i, k)\n",
    "        pos = edge_to_pos.get(e)\n",
    "        if pos is not None:\n",
    "            cot_sum[pos] += cot_angle_at_vertex(j, k, i)\n",
    "            cot_cnt[pos] += 1\n",
    "\n",
    "    # ---- finalize ----\n",
    "    # For a closed manifold triangle mesh, every edge should have cot_cnt==2.\n",
    "    weights = np.full(E, float(boundary_weight), dtype=np.float64)\n",
    "    interior = (cot_cnt == 2)\n",
    "    weights[interior] = cot_sum[interior]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def progressive_mesh(mesh, min_faces=4, randomization_percentage=0.95, expire_time=100):\n",
    "    # faces as (F,3) int array\n",
    "    current_faces = mesh.faces.reshape(-1, 4)[:, 1:]\n",
    "    current_points = mesh.points.copy()\n",
    "\n",
    "    # preallocate some extra room for new vertices\n",
    "    current_points = np.vstack(\n",
    "        [current_points, np.zeros((current_points.shape[0], 3))]\n",
    "    )\n",
    "    next_vid = mesh.n_points  # index for next new vertex\n",
    "\n",
    "    prog_face_del = []\n",
    "    prog_idx_replace = []\n",
    "    prog_weight = []\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    init_bool = True\n",
    "    while True:\n",
    "        if time.time() - start > expire_time:\n",
    "            print(\"expired...\")\n",
    "            break\n",
    "\n",
    "        # build undirected edge list from current_faces\n",
    "        edges = np.vstack(\n",
    "            [\n",
    "                current_faces[:, [0, 1]],\n",
    "                current_faces[:, [1, 2]],\n",
    "                current_faces[:, [0, 2]],\n",
    "            ]\n",
    "        )\n",
    "        edges = np.unique(np.sort(edges, axis=1), axis=0)\n",
    "        edge_array = edges\n",
    "\n",
    "        edge_lens = np.linalg.norm(\n",
    "            current_points[edge_array[:, 0]] - current_points[edge_array[:, 1]],\n",
    "            axis=1,\n",
    "        )\n",
    "        n_edge_remaining = len(edge_lens)\n",
    "        if init_bool:\n",
    "            edge_to_idx = {tuple(e): i for i, e in enumerate(edge_array)}\n",
    "            edge_dict, _, _ = make_edge_dict(mesh)\n",
    "            current_weights = np.repeat(-999, 4*n_edge_remaining).astype(float)\n",
    "            n_total_edges = n_edge_remaining\n",
    "            for e in edge_dict:\n",
    "                idx = edge_to_idx[e]\n",
    "                weight = edge_dict[e]['weight']\n",
    "                current_weights[idx] = weight\n",
    "            init_edges = n_edge_remaining\n",
    "            init_bool = False\n",
    "            pbar = tqdm(total=n_edge_remaining//3, desc='Edge Reduction', dynamic_ncols=True)\n",
    "\n",
    "        # stop if already super coarse\n",
    "        if current_faces.shape[0] <= min_faces or n_edge_remaining <= 6:\n",
    "            print(\"hit coarse stop:\", current_faces.shape[0], \"faces\")\n",
    "            break\n",
    "\n",
    "        edge_del_idx = np.argsort(edge_lens)\n",
    "        np.random.shuffle(edge_del_idx[:(n_edge_remaining // 5)]) if (n_edge_remaining / init_edges) > randomization_percentage else None\n",
    "        \n",
    "\n",
    "        accepted = False\n",
    "        vidx_0 = vidx_1 = None\n",
    "        fidx_del = None\n",
    "\n",
    "        for idx in edge_del_idx:\n",
    "            v0 = edge_array[idx, 0]\n",
    "            v1 = edge_array[idx, 1]\n",
    "\n",
    "            # simulate collapse v0,v1 -> next_vid\n",
    "            mask_edge_faces = (\n",
    "                np.sum(np.isin(current_faces, [v0, v1]), axis=1) == 2\n",
    "            )\n",
    "            fidx_del = np.where(mask_edge_faces)[0]\n",
    "            face_del_temp = current_faces[fidx_del]\n",
    "            faces_temp = np.delete(current_faces, fidx_del, axis=0)\n",
    "            faces_temp[np.isin(faces_temp, [v0, v1])] = next_vid\n",
    "\n",
    "            # 1) no degenerate triangles: 3 distinct verts per face\n",
    "            a, b, c = faces_temp.T\n",
    "            good_tri = (a != b) & (b != c) & (a != c)\n",
    "            if not np.all(good_tri):\n",
    "                continue\n",
    "\n",
    "            # 2) no duplicate faces (up to permutation)\n",
    "            ordered = np.sort(faces_temp, axis=1)\n",
    "            if np.unique(ordered, axis=0).shape[0] != faces_temp.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # 3) Euler characteristic stays 2 (genus-0)\n",
    "            V = np.unique(faces_temp).size\n",
    "            e_sim = np.vstack(\n",
    "                [\n",
    "                    faces_temp[:, [0, 1]],\n",
    "                    faces_temp[:, [1, 2]],\n",
    "                    faces_temp[:, [0, 2]],\n",
    "                ]\n",
    "            )\n",
    "            e_sim = np.unique(np.sort(e_sim, axis=1), axis=0)\n",
    "            E = e_sim.shape[0]\n",
    "            F = faces_temp.shape[0]\n",
    "            chi = V - E + F\n",
    "            if chi != 2:\n",
    "                continue\n",
    "\n",
    "            # if we get here, collapse is accepted\n",
    "            prog_face_del.append(((fidx_del[0], face_del_temp[0]), (fidx_del[1], face_del_temp[1])))\n",
    "            vidx_0, vidx_1 = v0, v1\n",
    "            replace_idx_0 = np.where(current_faces == vidx_0)\n",
    "            replace_idx_1 = np.where(current_faces == vidx_1)\n",
    "            prog_idx_replace.append(((replace_idx_0, vidx_0), (replace_idx_1, vidx_1)))\n",
    "            old_faces = current_faces[np.any(np.isin(current_faces, [vidx_0, vidx_1]), axis=1)]\n",
    "            current_faces = faces_temp\n",
    "            accepted = True\n",
    "            pbar.update(1)\n",
    "            break\n",
    "\n",
    "        if not accepted:\n",
    "            print(\"no valid collapse found; stopping\")\n",
    "            break\n",
    "\n",
    "        # create the new vertex position (midpoint of v0,v1)\n",
    "        mid = (current_points[vidx_0] + current_points[vidx_1]) / 2\n",
    "        current_points[next_vid] = mid\n",
    "        \n",
    "\n",
    "        rel_faces = current_faces[np.any(current_faces == next_vid, axis=1)]\n",
    "        rel_edges = np.vstack([rel_faces[:,[0,1]], rel_faces[:,[0,2]], rel_faces[:,[1,2]]])\n",
    "        rel_edges = np.unique(np.sort(rel_edges, axis=1), axis=0)\n",
    "        new_weights = cotan_weights_for_edge_list(rel_edges, current_points, current_faces)\n",
    "        weight_change_idx = []\n",
    "        prev_weights = []\n",
    "        for i in range(len(new_weights)):\n",
    "            edge_tuple = tuple(rel_edges[i])\n",
    "            weight = new_weights[i]\n",
    "            if edge_tuple not in edge_to_idx:\n",
    "                widx = n_total_edges\n",
    "                n_total_edges += 1\n",
    "                if n_total_edges == len(current_weights):\n",
    "                    current_weights = np.hstack([current_weights, np.repeat(-999, 1000)])\n",
    "                edge_to_idx[edge_tuple] = widx\n",
    "            else:\n",
    "                widx = edge_to_idx[edge_tuple]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = weight\n",
    "            weight_change_idx.append(widx)\n",
    "\n",
    "        old_edges = np.vstack([old_faces[:,[0,1]], old_faces[:,[0,2]], old_faces[:,[1,2]]])\n",
    "        old_edges = np.unique(np.sort(old_edges, axis=1), axis=0)\n",
    "        old_edges = old_edges[np.any(np.isin(old_edges, [vidx_0, vidx_1]), axis=1)]\n",
    "        old_edges_idx = []\n",
    "        for e in old_edges:\n",
    "            widx = edge_to_idx[tuple(e)]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = 0\n",
    "            old_edges_idx.append(widx)\n",
    "        undo = (weight_change_idx + old_edges_idx, prev_weights)\n",
    "        prog_weight.append(undo)\n",
    "        # prog_weight.append(((np.array(weight_change_idx), new_weights), (np.array(old_edges_idx), np.zeros(len(old_edges_idx)))))\n",
    "        next_vid += 1\n",
    "\n",
    "        # expand point buffer if needed\n",
    "        if next_vid == current_points.shape[0]:\n",
    "            current_points = np.vstack(\n",
    "                [current_points, np.zeros((50, 3))]\n",
    "            )\n",
    "\n",
    "    zeros = np.all(current_points == 0, axis=1)\n",
    "    if np.any(zeros):\n",
    "        current_points = current_points[:np.argmax(zeros)]\n",
    "    if np.any(current_weights == -999):\n",
    "        current_weights = current_weights[:np.argmin(current_weights)]\n",
    "    print(f\"lifetime edges: {len(current_weights)}!\")\n",
    "    return current_points, current_faces, prog_idx_replace[::-1], prog_face_del[::-1], edge_to_idx, current_weights, prog_weight[::-1]\n",
    "\n",
    "def repulsion_force(points, edges=None, edge_cancel=True):\n",
    "    \"\"\"\n",
    "    points: (N,3) on S^2\n",
    "    edges:  (E,2) int or None\n",
    "    returns (N,3) total repulsion per vertex\n",
    "    \"\"\"\n",
    "    N = points.shape[0]\n",
    "    diff = points[:, None, :] - points[None, :, :]          # (N,N,3)\n",
    "    r2 = np.sum(diff * diff, axis=2, keepdims=True) + 1e-8  # (N,N,1)\n",
    "    np.fill_diagonal(r2[:, :, 0], np.inf)\n",
    "    F = diff / r2                                           # 1/r^2 repulsion\n",
    "\n",
    "    if edge_cancel and edges is not None:\n",
    "        F[edges[:, 0], edges[:, 1]] = 0.0\n",
    "        F[edges[:, 1], edges[:, 0]] = 0.0\n",
    "\n",
    "    return F.sum(axis=1)                                    # (N,3)\n",
    "\n",
    "def check_spherical_flips(phi, faces):\n",
    "    \"\"\"\n",
    "    Returns True if ANY face has flipped or collapsed (signed volume <= 0).\n",
    "    Uses the scalar triple product: (p0 x p1) . p2\n",
    "    \"\"\"\n",
    "    p0 = phi[faces[:, 0]]\n",
    "    p1 = phi[faces[:, 1]]\n",
    "    p2 = phi[faces[:, 2]]\n",
    "    \n",
    "    # Vectorized Triple Product\n",
    "    # Cross product of first two edges\n",
    "    cross = np.cross(p0, p1)\n",
    "    \n",
    "    # Dot product with third vertex\n",
    "    # Result is ~6x the signed volume of the tetrahedron formed with the origin\n",
    "    volumes = np.einsum('ij,ij->i', cross, p2)\n",
    "    \n",
    "    # If any volume is <= epsilon, it's degenerate or flipped\n",
    "    # Using 1e-12 as a safety margin against float errors\n",
    "    if np.any(volumes < 1e-12):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def C2_adaptive(phi0, edges_idx, weights, faces, mask=None, *,\n",
    "                tol=1e-10, max_iter=10000, initial_dt=0.1, verbose=True, \n",
    "                repulsion=False):\n",
    "    \"\"\"\n",
    "    Drop-in replacement with FACE FLIP CHECKING.\n",
    "    Added 'faces' argument to signature.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=max_iter, desc='Spherical Param', unit='iter', dynamic_ncols=True)\n",
    "        \n",
    "    phi = phi0.copy()\n",
    "    \n",
    "    # Normalize initial active points\n",
    "    if mask is not None:\n",
    "        phi[mask] /= (np.linalg.norm(phi[mask], axis=1, keepdims=True) + 1e-15)\n",
    "    else:\n",
    "        phi /= (np.linalg.norm(phi, axis=1, keepdims=True) + 1e-15)\n",
    "\n",
    "    E = string_energy_vec(edges_idx, weights, phi)\n",
    "    energies = [E]\n",
    "    dt = float(initial_dt)\n",
    "\n",
    "    # Auto-balance repulsion\n",
    "    if repulsion:\n",
    "        n_active = np.sum(mask) if mask is not None else len(phi)\n",
    "        repulsion_weight = 50.0 / (n_active if n_active > 0 else 1.0)\n",
    "        \n",
    "        # Pre-calculate inverse mask for safety clamp\n",
    "        if mask is not None:\n",
    "            inv_mask = ~mask\n",
    "\n",
    "    for it in range(1, max_iter+1):\n",
    "        g = d_energy_vec(edges_idx, weights, phi)\n",
    "        \n",
    "        if repulsion:\n",
    "            # Calculate repulsion only on ACTIVE points\n",
    "            # (Passing mask to repulsion_force if you optimized it, \n",
    "            #  or just slicing like this if using your original function)\n",
    "            rf_active = repulsion_force(phi[mask])\n",
    "            \n",
    "            # SUBTRACT repulsion (Gradient Descent moves opposite to g)\n",
    "            g[mask] -= (rf_active * repulsion_weight)\n",
    "            \n",
    "        # Project gradient\n",
    "        g_proj = g - (np.einsum('ij,ij->i', g, phi))[:, None] * phi\n",
    "\n",
    "        ok = False\n",
    "        dt_try = dt\n",
    "        \n",
    "        # Line Search\n",
    "        for _ in range(8): # Increased tries slightly to find valid non-flipping steps\n",
    "            phi_try = phi - dt_try * g_proj\n",
    "            \n",
    "            # --- SAFETY CLAMP & CENTERING ---\n",
    "            if mask is not None:\n",
    "                phi_try[inv_mask] = 0.0 # Force ghosts dead\n",
    "                \n",
    "                # Center active points\n",
    "                center = np.sum(phi_try, axis=0) / np.sum(mask)\n",
    "                phi_try[mask] -= center\n",
    "                \n",
    "                # Normalize active points\n",
    "                norms = np.linalg.norm(phi_try, axis=1)\n",
    "                safe_norm_mask = norms > 1e-15\n",
    "                phi_try[safe_norm_mask] /= norms[safe_norm_mask][:,None]\n",
    "            else:\n",
    "                phi_try -= np.mean(phi_try, axis=0)\n",
    "                phi_try /= (np.linalg.norm(phi_try, axis=1, keepdims=True) + 1e-15)\n",
    "            # --------------------------------\n",
    "\n",
    "            # 1. Check Energy\n",
    "            E_try = string_energy_vec(edges_idx, weights, phi_try)\n",
    "            \n",
    "            if E_try < E:\n",
    "                # 2. CRITICAL: Check for Flipped Faces\n",
    "                # We only check if the energy looks good.\n",
    "                # If energy improves but faces flip, we MUST REJECT.\n",
    "                if check_spherical_flips(phi_try, faces):\n",
    "                    # Flipped! Reject this step size.\n",
    "                    dt_try *= 0.5\n",
    "                    continue\n",
    "                \n",
    "                # If we get here: Energy is lower AND no faces flipped. Accept.\n",
    "                phi, E = phi_try, E_try\n",
    "                energies.append(E)\n",
    "                dt = min(dt_try * 1.25, 1.0) # Grow step size carefully\n",
    "                ok = True\n",
    "                break\n",
    "                \n",
    "            dt_try *= 0.5 # Energy didn't improve, shrink step\n",
    "\n",
    "        if not ok or (len(energies) > 1 and abs(energies[-2] - energies[-1]) < tol):\n",
    "            break\n",
    "        if verbose:\n",
    "            pbar.update(1)\n",
    "            \n",
    "    return phi, energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a57f4b4-9b84-4048-bb4f-9d83d006e50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ba569daaca49818b932a0583751e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Edge Reduction:   0%|                                                                         | 0/3391 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit coarse stop: 4 faces\n",
      "lifetime edges: 34203!\n"
     ]
    }
   ],
   "source": [
    "cow, horse, shark2, shark = load_pickle('cow_horse_shark2_shark_processed_meshes')\n",
    "boio3 = progressive_mesh(horse, expire_time=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8cb5f5-a916-42df-af8d-2a1387fdb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 30\n",
    "c2_iter= 3000\n",
    "step_per_plot = 1\n",
    "repul = True\n",
    "\n",
    "# ---------- main visualization ----------\n",
    "\n",
    "points, faces_i, prog_point, prog_face, edge_to_idx, weights_i, prog_edge = boio3\n",
    "full_edge_array = np.array(list(edge_to_idx.keys()))\n",
    "# rescale horse just so it fits nicely\n",
    "points = points.copy()\n",
    "points *= 2 / np.max(np.max(points, axis=0) - np.min(points, axis=0))\n",
    "points -= (np.max(points, axis=0) - 1)\n",
    "\n",
    "width = 2.0\n",
    "height = 2.0\n",
    "\n",
    "# spherical positions (full N, but many unused initially)\n",
    "spho = np.zeros_like(points)\n",
    "# start coarse sphere as tetrahedron (pyvista platonic solid: 0=tetra)\n",
    "spho[np.unique(faces_i)] = pv.PlatonicSolid(0).points\n",
    "active_point_mask = np.zeros(spho.shape[0], dtype=bool)\n",
    "\n",
    "p = BackgroundPlotter()\n",
    "h = 0\n",
    "ho = np.array([0.0, width * 1.1, 0.0])\n",
    "v = 0\n",
    "vo = np.array([0.0, 0.0, height * 1.1])\n",
    "\n",
    "for step in range(n_step):\n",
    "    propo = prog_point[step]\n",
    "    profa = prog_face[step]\n",
    "    proed = prog_edge[step]\n",
    "\n",
    "    faces_i, r0, r1, repo = split_vertex_insert(\n",
    "        faces_i, profa, propo, return_replaced_idx=True\n",
    "    )\n",
    "\n",
    "    weight_idx = np.array(proed[0])\n",
    "    weight_val = np.array(proed[1])\n",
    "    weight_val[weight_val < -998] = 0\n",
    "    weights_i[weight_idx] = weight_val\n",
    "\n",
    "    # 2) horse mesh at this resolution (green)\n",
    "    hpnts, hfaces, _ = remove_unused_points(points, faces_i)\n",
    "    if step % step_per_plot == 0:\n",
    "        horse_mesh = pv.PolyData(hpnts, append_three_col(hfaces))\n",
    "        horse_mesh.points += h*ho + v*vo\n",
    "        p.add_mesh(horse_mesh, color='seagreen', show_edges=True, line_width=1)\n",
    "\n",
    "    # 3) spherical mesh BEFORE relaxation (pink)\n",
    "    spnts, sfaces, global_idx = remove_unused_points(spho, faces_i)\n",
    "    propo_0_rep_idx = np.where(global_idx == propo[0][1])[0][0]\n",
    "    propo_1_rep_idx = np.where(global_idx == propo[1][1])[0][0]\n",
    "    og_point = spho[repo]\n",
    "    init = initialize_point(spnts, sfaces, propo_0_rep_idx, propo_1_rep_idx, og_point, n_iter=50000)\n",
    "    spnts[propo_0_rep_idx] = init[propo_0_rep_idx]\n",
    "    spnts[propo_1_rep_idx] = init[propo_1_rep_idx]\n",
    "\n",
    "    if step % step_per_plot == 0:\n",
    "        spho_before = pv.PolyData(spnts + h*ho + v*vo + np.array([-4.0, 0, 0]),\n",
    "                                  append_three_col(sfaces))\n",
    "        p.add_mesh(spho_before, color='lightcoral', show_edges=True, line_width=1)\n",
    "\n",
    "\n",
    "    spho[global_idx] = spnts\n",
    "    active_mask = weights_i > 0\n",
    "    active_weights_i = weights_i[active_mask]\n",
    "    active_edge_array_i = full_edge_array[active_mask]\n",
    "    \n",
    "    active_point_mask[global_idx] = True\n",
    "    \n",
    "    spho, energ = C2_adaptive(spho, active_edge_array_i, active_weights_i, faces_i, max_iter=c2_iter, verbose=False, repulsion=repul, mask=active_point_mask)\n",
    "\n",
    "    active_point_mask[global_idx] = False\n",
    "    \n",
    "    # write back updated spherical points\n",
    "    # spho[global_idx] = pts\n",
    "\n",
    "    if step % step_per_plot == 0:\n",
    "        # grid layout\n",
    "        h += 1\n",
    "        if h % 5 == 0:\n",
    "            h = 0\n",
    "            v += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258b3b6-3ced-4a18-a11f-4384a02298de",
   "metadata": {},
   "source": [
    "# TESTING OTHER PROGRESSIVE MESH METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bf806-2ee7-42a0-adf0-313163066947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_bro(prog_output, n_max=None, n=25, n_row=5, ppcolor='seagreen'):\n",
    "    points, faces_i, prog_point, prog_face, edge_to_idx, weights_i, prog_edge = prog_output\n",
    "    n_max = len(prog_point) if n_max is None else n_max\n",
    "    n_plot = np.linspace(0, n_max, n).astype(int)\n",
    "\n",
    "    points = points.copy()\n",
    "    points *= 2 / np.max(np.max(points, axis=0) - np.min(points, axis=0))\n",
    "    points -= (np.max(points, axis=0) - 1)\n",
    "\n",
    "    width = 2.0\n",
    "    height = 2.0\n",
    "\n",
    "    p = BackgroundPlotter()\n",
    "    h = 0\n",
    "    ho = np.array([0.0, width*1.1, 0.0])\n",
    "    v = 0\n",
    "    vo = np.array([0.0, 0.0, height * 1.1])\n",
    "\n",
    "    for step in range(len(prog_point)):\n",
    "        propo = prog_point[step]\n",
    "        profa = prog_face[step]\n",
    "        proed = prog_edge[step]\n",
    "    \n",
    "        faces_i, r0, r1, repo = split_vertex_insert(\n",
    "            faces_i, profa, propo, return_replaced_idx=True\n",
    "        )\n",
    "        if step in n_plot:\n",
    "            hpnts, hfaces, _ = remove_unused_points(points, faces_i)\n",
    "            horse_mesh = pv.PolyData(hpnts, append_three_col(hfaces))\n",
    "            horse_mesh.points += h*ho + v*vo\n",
    "            p.add_mesh(horse_mesh, color=ppcolor, show_edges=True, line_width=1)\n",
    "            h += 1\n",
    "            if h % n_row == 0:\n",
    "                h = 0\n",
    "                v += 1\n",
    "    return 'poop'\n",
    "\n",
    "def progressive_mesh_deg(mesh, min_faces=4, randomization_percentage=0.95, expire_time=100):\n",
    "    centy = mesh.center_of_mass()\n",
    "    # faces as (F,3) int array\n",
    "    current_faces = mesh.faces.reshape(-1, 4)[:, 1:]\n",
    "    current_points = mesh.points.copy()\n",
    "\n",
    "    # preallocate some extra room for new vertices\n",
    "    current_points = np.vstack(\n",
    "        [current_points, np.zeros((current_points.shape[0], 3))]\n",
    "    )\n",
    "    next_vid = mesh.n_points  # index for next new vertex\n",
    "\n",
    "    prog_face_del = []\n",
    "    prog_idx_replace = []\n",
    "    prog_weight = []\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    init_bool = True\n",
    "    while True:\n",
    "        if time.time() - start > expire_time:\n",
    "            print(\"expired...\")\n",
    "            break\n",
    "\n",
    "        # build undirected edge list from current_faces\n",
    "        edges = np.vstack(\n",
    "            [\n",
    "                current_faces[:, [0, 1]],\n",
    "                current_faces[:, [1, 2]],\n",
    "                current_faces[:, [0, 2]],\n",
    "            ]\n",
    "        )\n",
    "        edges = np.unique(np.sort(edges, axis=1), axis=0)\n",
    "        edge_array = edges\n",
    "\n",
    "        edge_lens = np.linalg.norm(\n",
    "            current_points[edge_array[:, 0]] - current_points[edge_array[:, 1]],\n",
    "            axis=1,\n",
    "        )\n",
    "        n_edge_remaining = len(edge_lens)\n",
    "        if init_bool:\n",
    "            edge_to_idx = {tuple(e): i for i, e in enumerate(edge_array)}\n",
    "            edge_dict, _, _ = make_edge_dict(mesh)\n",
    "            current_weights = np.repeat(-999, 4*n_edge_remaining).astype(float)\n",
    "            n_total_edges = n_edge_remaining\n",
    "            for e in edge_dict:\n",
    "                idx = edge_to_idx[e]\n",
    "                weight = edge_dict[e]['weight']\n",
    "                current_weights[idx] = weight\n",
    "            init_edges = n_edge_remaining\n",
    "            init_bool = False\n",
    "            pbar = tqdm(total=n_edge_remaining//3, desc='Edge Reduction', dynamic_ncols=True)\n",
    "\n",
    "        # stop if already super coarse\n",
    "        if current_faces.shape[0] <= min_faces or n_edge_remaining <= 6:\n",
    "            print(\"hit coarse stop:\", current_faces.shape[0], \"faces\")\n",
    "            break\n",
    "\n",
    "        #edge_del_idx = np.argsort(edge_lens)\n",
    "        #np.random.shuffle(edge_del_idx[:(n_edge_remaining // 5)]) if (n_edge_remaining / init_edges) > randomization_percentage else None\n",
    "        # degris:\n",
    "        # 1. Get degrees of all vertices\n",
    "        deggy = np.bincount(np.ravel(current_faces))\n",
    "        \n",
    "        # 2. Calculate a score for every edge\n",
    "        # This adds the degrees of both endpoints and multiplies by length\n",
    "        # High degree + Long edge = Massive score (Top of the list)\n",
    "        edge_scores = (deggy[edge_array[:, 0]] + deggy[edge_array[:, 1]]) * edge_lens\n",
    "        \n",
    "        # 3. Create the flat edge_del_idx\n",
    "        edge_del_idx = np.argsort(edge_scores)[::-1]\n",
    "\n",
    "        accepted = False\n",
    "        vidx_0 = vidx_1 = None\n",
    "        fidx_del = None\n",
    "\n",
    "        for idx in edge_del_idx:\n",
    "            v0 = edge_array[idx, 0]\n",
    "            v1 = edge_array[idx, 1]\n",
    "\n",
    "            # simulate collapse v0,v1 -> next_vid\n",
    "            mask_edge_faces = (\n",
    "                np.sum(np.isin(current_faces, [v0, v1]), axis=1) == 2\n",
    "            )\n",
    "            fidx_del = np.where(mask_edge_faces)[0]\n",
    "            face_del_temp = current_faces[fidx_del]\n",
    "            faces_temp = np.delete(current_faces, fidx_del, axis=0)\n",
    "            faces_temp[np.isin(faces_temp, [v0, v1])] = next_vid\n",
    "\n",
    "            # 1) no degenerate triangles: 3 distinct verts per face\n",
    "            a, b, c = faces_temp.T\n",
    "            good_tri = (a != b) & (b != c) & (a != c)\n",
    "            if not np.all(good_tri):\n",
    "                continue\n",
    "\n",
    "            # 2) no duplicate faces (up to permutation)\n",
    "            ordered = np.sort(faces_temp, axis=1)\n",
    "            if np.unique(ordered, axis=0).shape[0] != faces_temp.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # 3) Euler characteristic stays 2 (genus-0)\n",
    "            V = np.unique(faces_temp).size\n",
    "            e_sim = np.vstack(\n",
    "                [\n",
    "                    faces_temp[:, [0, 1]],\n",
    "                    faces_temp[:, [1, 2]],\n",
    "                    faces_temp[:, [0, 2]],\n",
    "                ]\n",
    "            )\n",
    "            e_sim = np.unique(np.sort(e_sim, axis=1), axis=0)\n",
    "            E = e_sim.shape[0]\n",
    "            F = faces_temp.shape[0]\n",
    "            chi = V - E + F\n",
    "            if chi != 2:\n",
    "                continue\n",
    "\n",
    "            # if we get here, collapse is accepted\n",
    "            prog_face_del.append(((fidx_del[0], face_del_temp[0]), (fidx_del[1], face_del_temp[1])))\n",
    "            vidx_0, vidx_1 = v0, v1\n",
    "            replace_idx_0 = np.where(current_faces == vidx_0)\n",
    "            replace_idx_1 = np.where(current_faces == vidx_1)\n",
    "            prog_idx_replace.append(((replace_idx_0, vidx_0), (replace_idx_1, vidx_1)))\n",
    "            old_faces = current_faces[np.any(np.isin(current_faces, [vidx_0, vidx_1]), axis=1)]\n",
    "            current_faces = faces_temp\n",
    "            accepted = True\n",
    "            pbar.update(1)\n",
    "            break\n",
    "\n",
    "        if not accepted:\n",
    "            print(\"no valid collapse found; stopping\")\n",
    "            break\n",
    "\n",
    "        # create the new vertex position (midpoint of v0,v1)\n",
    "        mid = (current_points[vidx_0] + current_points[vidx_1]) / 2\n",
    "        current_points[next_vid] = mid\n",
    "        \n",
    "\n",
    "        rel_faces = current_faces[np.any(current_faces == next_vid, axis=1)]\n",
    "        rel_edges = np.vstack([rel_faces[:,[0,1]], rel_faces[:,[0,2]], rel_faces[:,[1,2]]])\n",
    "        rel_edges = np.unique(np.sort(rel_edges, axis=1), axis=0)\n",
    "        new_weights = cotan_weights_for_edge_list(rel_edges, current_points, current_faces)\n",
    "        weight_change_idx = []\n",
    "        prev_weights = []\n",
    "        for i in range(len(new_weights)):\n",
    "            edge_tuple = tuple(rel_edges[i])\n",
    "            weight = new_weights[i]\n",
    "            if edge_tuple not in edge_to_idx:\n",
    "                widx = n_total_edges\n",
    "                n_total_edges += 1\n",
    "                if n_total_edges == len(current_weights):\n",
    "                    current_weights = np.hstack([current_weights, np.repeat(-999, 1000)])\n",
    "                edge_to_idx[edge_tuple] = widx\n",
    "            else:\n",
    "                widx = edge_to_idx[edge_tuple]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = weight\n",
    "            weight_change_idx.append(widx)\n",
    "\n",
    "        old_edges = np.vstack([old_faces[:,[0,1]], old_faces[:,[0,2]], old_faces[:,[1,2]]])\n",
    "        old_edges = np.unique(np.sort(old_edges, axis=1), axis=0)\n",
    "        old_edges = old_edges[np.any(np.isin(old_edges, [vidx_0, vidx_1]), axis=1)]\n",
    "        old_edges_idx = []\n",
    "        for e in old_edges:\n",
    "            widx = edge_to_idx[tuple(e)]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = 0\n",
    "            old_edges_idx.append(widx)\n",
    "        undo = (weight_change_idx + old_edges_idx, prev_weights)\n",
    "        prog_weight.append(undo)\n",
    "        # prog_weight.append(((np.array(weight_change_idx), new_weights), (np.array(old_edges_idx), np.zeros(len(old_edges_idx)))))\n",
    "        next_vid += 1\n",
    "\n",
    "        # expand point buffer if needed\n",
    "        if next_vid == current_points.shape[0]:\n",
    "            current_points = np.vstack(\n",
    "                [current_points, np.zeros((50, 3))]\n",
    "            )\n",
    "\n",
    "    zeros = np.all(current_points == 0, axis=1)\n",
    "    if np.any(zeros):\n",
    "        current_points = current_points[:np.argmax(zeros)]\n",
    "    if np.any(current_weights == -999):\n",
    "        current_weights = current_weights[:np.argmin(current_weights)]\n",
    "    print(f\"lifetime edges: {len(current_weights)}!\")\n",
    "    return current_points, current_faces, prog_idx_replace[::-1], prog_face_del[::-1], edge_to_idx, current_weights, prog_weight[::-1]\n",
    "\n",
    "def progressive_mesh_deg2(mesh, min_faces=4, randomization_percentage=0.95, expire_time=100):\n",
    "    centy = mesh.center_of_mass()\n",
    "    # faces as (F,3) int array\n",
    "    current_faces = mesh.faces.reshape(-1, 4)[:, 1:]\n",
    "    current_points = mesh.points.copy()\n",
    "\n",
    "    # preallocate some extra room for new vertices\n",
    "    current_points = np.vstack(\n",
    "        [current_points, np.zeros((current_points.shape[0], 3))]\n",
    "    )\n",
    "    next_vid = mesh.n_points  # index for next new vertex\n",
    "\n",
    "    prog_face_del = []\n",
    "    prog_idx_replace = []\n",
    "    prog_weight = []\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    init_bool = True\n",
    "    while True:\n",
    "        if time.time() - start > expire_time:\n",
    "            print(\"expired...\")\n",
    "            break\n",
    "\n",
    "        # build undirected edge list from current_faces\n",
    "        edges = np.vstack(\n",
    "            [\n",
    "                current_faces[:, [0, 1]],\n",
    "                current_faces[:, [1, 2]],\n",
    "                current_faces[:, [0, 2]],\n",
    "            ]\n",
    "        )\n",
    "        edges = np.unique(np.sort(edges, axis=1), axis=0)\n",
    "        edge_array = edges\n",
    "\n",
    "        edge_lens = np.linalg.norm(\n",
    "            current_points[edge_array[:, 0]] - current_points[edge_array[:, 1]],\n",
    "            axis=1,\n",
    "        )\n",
    "        n_edge_remaining = len(edge_lens)\n",
    "        if init_bool:\n",
    "            edge_to_idx = {tuple(e): i for i, e in enumerate(edge_array)}\n",
    "            edge_dict, _, _ = make_edge_dict(mesh)\n",
    "            current_weights = np.repeat(-999, 4*n_edge_remaining).astype(float)\n",
    "            n_total_edges = n_edge_remaining\n",
    "            for e in edge_dict:\n",
    "                idx = edge_to_idx[e]\n",
    "                weight = edge_dict[e]['weight']\n",
    "                current_weights[idx] = weight\n",
    "            init_edges = n_edge_remaining\n",
    "            init_bool = False\n",
    "            pbar = tqdm(total=n_edge_remaining//3, desc='Edge Reduction', dynamic_ncols=True)\n",
    "\n",
    "        # stop if already super coarse\n",
    "        if current_faces.shape[0] <= min_faces or n_edge_remaining <= 6:\n",
    "            print(\"hit coarse stop:\", current_faces.shape[0], \"faces\")\n",
    "            break\n",
    "\n",
    "        #edge_del_idx = np.argsort(edge_lens)\n",
    "        #np.random.shuffle(edge_del_idx[:(n_edge_remaining // 5)]) if (n_edge_remaining / init_edges) > randomization_percentage else None\n",
    "        # degris:\n",
    "        # 1. Get degrees of all vertices\n",
    "        deggy = np.bincount(np.ravel(current_faces))\n",
    "        \n",
    "        # 2. Calculate a score for every edge\n",
    "        # This adds the degrees of both endpoints and multiplies by length\n",
    "        # High degree + Long edge = Massive score (Top of the list)\n",
    "        edge_scores = (deggy[edge_array[:, 0]] + deggy[edge_array[:, 1]]) * edge_lens\n",
    "        \n",
    "        # 3. Create the flat edge_del_idx\n",
    "        edge_del_idx = np.argsort(edge_scores)\n",
    "\n",
    "        accepted = False\n",
    "        vidx_0 = vidx_1 = None\n",
    "        fidx_del = None\n",
    "\n",
    "        for idx in edge_del_idx:\n",
    "            v0 = edge_array[idx, 0]\n",
    "            v1 = edge_array[idx, 1]\n",
    "\n",
    "            # simulate collapse v0,v1 -> next_vid\n",
    "            mask_edge_faces = (\n",
    "                np.sum(np.isin(current_faces, [v0, v1]), axis=1) == 2\n",
    "            )\n",
    "            fidx_del = np.where(mask_edge_faces)[0]\n",
    "            face_del_temp = current_faces[fidx_del]\n",
    "            faces_temp = np.delete(current_faces, fidx_del, axis=0)\n",
    "            faces_temp[np.isin(faces_temp, [v0, v1])] = next_vid\n",
    "\n",
    "            # 1) no degenerate triangles: 3 distinct verts per face\n",
    "            a, b, c = faces_temp.T\n",
    "            good_tri = (a != b) & (b != c) & (a != c)\n",
    "            if not np.all(good_tri):\n",
    "                continue\n",
    "\n",
    "            # 2) no duplicate faces (up to permutation)\n",
    "            ordered = np.sort(faces_temp, axis=1)\n",
    "            if np.unique(ordered, axis=0).shape[0] != faces_temp.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # 3) Euler characteristic stays 2 (genus-0)\n",
    "            V = np.unique(faces_temp).size\n",
    "            e_sim = np.vstack(\n",
    "                [\n",
    "                    faces_temp[:, [0, 1]],\n",
    "                    faces_temp[:, [1, 2]],\n",
    "                    faces_temp[:, [0, 2]],\n",
    "                ]\n",
    "            )\n",
    "            e_sim = np.unique(np.sort(e_sim, axis=1), axis=0)\n",
    "            E = e_sim.shape[0]\n",
    "            F = faces_temp.shape[0]\n",
    "            chi = V - E + F\n",
    "            if chi != 2:\n",
    "                continue\n",
    "\n",
    "            # if we get here, collapse is accepted\n",
    "            prog_face_del.append(((fidx_del[0], face_del_temp[0]), (fidx_del[1], face_del_temp[1])))\n",
    "            vidx_0, vidx_1 = v0, v1\n",
    "            replace_idx_0 = np.where(current_faces == vidx_0)\n",
    "            replace_idx_1 = np.where(current_faces == vidx_1)\n",
    "            prog_idx_replace.append(((replace_idx_0, vidx_0), (replace_idx_1, vidx_1)))\n",
    "            old_faces = current_faces[np.any(np.isin(current_faces, [vidx_0, vidx_1]), axis=1)]\n",
    "            current_faces = faces_temp\n",
    "            accepted = True\n",
    "            pbar.update(1)\n",
    "            break\n",
    "\n",
    "        if not accepted:\n",
    "            print(\"no valid collapse found; stopping\")\n",
    "            break\n",
    "\n",
    "        # create the new vertex position (midpoint of v0,v1)\n",
    "        mid = (current_points[vidx_0] + current_points[vidx_1]) / 2\n",
    "        current_points[next_vid] = mid\n",
    "        \n",
    "\n",
    "        rel_faces = current_faces[np.any(current_faces == next_vid, axis=1)]\n",
    "        rel_edges = np.vstack([rel_faces[:,[0,1]], rel_faces[:,[0,2]], rel_faces[:,[1,2]]])\n",
    "        rel_edges = np.unique(np.sort(rel_edges, axis=1), axis=0)\n",
    "        new_weights = cotan_weights_for_edge_list(rel_edges, current_points, current_faces)\n",
    "        weight_change_idx = []\n",
    "        prev_weights = []\n",
    "        for i in range(len(new_weights)):\n",
    "            edge_tuple = tuple(rel_edges[i])\n",
    "            weight = new_weights[i]\n",
    "            if edge_tuple not in edge_to_idx:\n",
    "                widx = n_total_edges\n",
    "                n_total_edges += 1\n",
    "                if n_total_edges == len(current_weights):\n",
    "                    current_weights = np.hstack([current_weights, np.repeat(-999, 1000)])\n",
    "                edge_to_idx[edge_tuple] = widx\n",
    "            else:\n",
    "                widx = edge_to_idx[edge_tuple]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = weight\n",
    "            weight_change_idx.append(widx)\n",
    "\n",
    "        old_edges = np.vstack([old_faces[:,[0,1]], old_faces[:,[0,2]], old_faces[:,[1,2]]])\n",
    "        old_edges = np.unique(np.sort(old_edges, axis=1), axis=0)\n",
    "        old_edges = old_edges[np.any(np.isin(old_edges, [vidx_0, vidx_1]), axis=1)]\n",
    "        old_edges_idx = []\n",
    "        for e in old_edges:\n",
    "            widx = edge_to_idx[tuple(e)]\n",
    "            prev_weights.append(current_weights[widx])\n",
    "            current_weights[widx] = 0\n",
    "            old_edges_idx.append(widx)\n",
    "        undo = (weight_change_idx + old_edges_idx, prev_weights)\n",
    "        prog_weight.append(undo)\n",
    "        # prog_weight.append(((np.array(weight_change_idx), new_weights), (np.array(old_edges_idx), np.zeros(len(old_edges_idx)))))\n",
    "        next_vid += 1\n",
    "\n",
    "        # expand point buffer if needed\n",
    "        if next_vid == current_points.shape[0]:\n",
    "            current_points = np.vstack(\n",
    "                [current_points, np.zeros((50, 3))]\n",
    "            )\n",
    "\n",
    "    zeros = np.all(current_points == 0, axis=1)\n",
    "    if np.any(zeros):\n",
    "        current_points = current_points[:np.argmax(zeros)]\n",
    "    if np.any(current_weights == -999):\n",
    "        current_weights = current_weights[:np.argmin(current_weights)]\n",
    "    print(f\"lifetime edges: {len(current_weights)}!\")\n",
    "    return current_points, current_faces, prog_idx_replace[::-1], prog_face_del[::-1], edge_to_idx, current_weights, prog_weight[::-1]\n",
    "\n",
    "# selecting on lowest average degree of the two vertices of an edge works much better than choosing by highest!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
